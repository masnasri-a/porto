{
    "status": "ok",
    "feed": {
      "url": "https://medium.com/feed/@nasriadzlani",
      "title": "Stories by Nasri Adzlani on Medium",
      "link": "https://medium.com/@nasriadzlani?source=rss-6433ae88e0c6------2",
      "author": "",
      "description": "Stories by Nasri Adzlani on Medium",
      "image": "https://cdn-images-1.medium.com/fit/c/150/150/0*gwlwgarcFGP5cGII"
    },
    "items": [
      {
        "title": "Building a Data-Driven Application with LangChain, ChromaDB, and Llama Models",
        "pubDate": "2024-10-26 16:31:15",
        "link": "https://nasriadzlani.medium.com/building-a-data-driven-application-with-langchain-chromadb-and-llama-models-f75f2f77888f?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/f75f2f77888f",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Ch3\u003ETable of Contents\u003C/h3\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003EIntroduction\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ETechnologies Overview\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EStep-by-Step Code Walkthrough\u003C/strong\u003E\u003Cbr\u003E3.1. Preparing the Environment\u003Cbr\u003E3.2. Loading and Embedding Data\u003Cbr\u003E3.3. Creating a Vector Database with ChromaDB\u003Cbr\u003E3.4. Designing a Prompt Template\u003Cbr\u003E3.5. Running the LangChain Processing Pipeline\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EUse Case Applications\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EChallenges and Best Practices\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EConclusion\u003C/strong\u003E\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/720/0*GzFIxT6Svov-EKHe.png\"\u003E\u003Cfigcaption\u003ELangchain Logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch3\u003E1. Introduction\u003C/h3\u003E\n\u003Cp\u003EData-driven applications are becoming essential in various domains, from customer service to data analysis. By combining LangChain’s modular framework with a powerful local vector database like ChromaDB and leveraging state-of-the-art models like Llama 3.2, we can build a flexible solution that integrates data retrieval and large language models (LLMs).\u003C/p\u003E\n\u003Cp\u003EIn this article, we’ll walk through how to build a pipeline that processes CSV data, creates embeddings, and answers queries efficiently using the LangChain framework.\u003C/p\u003E\n\u003Ch3\u003E2. Technologies Overview\u003C/h3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ELangChain\u003C/strong\u003E: A versatile library for building applications that involve LLMs. It allows you to chain together different components like prompts, models, and outputs.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EChromaDB\u003C/strong\u003E: A local vector storage engine used to store and retrieve documents based on their embeddings.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ELlama 3.2\u003C/strong\u003E: A private language model designed for complex reasoning and text-based tasks.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ECSVLoader\u003C/strong\u003E: A utility to load CSV datasets directly into LangChain workflows.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EEmbeddings\u003C/strong\u003E: A numerical representation of text used to measure similarity between documents or queries.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E3. Step-by-Step Code Walkthrough\u003C/h3\u003E\n\u003Ch3\u003E3.1. Preparing the Environment\u003C/h3\u003E\n\u003Cp\u003EFirst, we import the necessary libraries. This setup will enable us to load datasets, manage embeddings, and create a seamless data pipeline.\u003C/p\u003E\n\u003Cpre\u003Efrom langchain.vectorstores import Chroma\u003Cbr\u003Efrom langchain.prompts import ChatPromptTemplate\u003Cbr\u003Efrom langchain_core.output_parsers import StrOutputParser\u003Cbr\u003Efrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\u003Cbr\u003Efrom langchain_ollama.llms import OllamaLLM\u003Cbr\u003Efrom langchain_community.document_loaders.csv_loader import CSVLoader\u003Cbr\u003Eimport os\u003Cbr\u003Efrom pathlib import Path\u003Cbr\u003Efrom langchain_ollama import OllamaEmbeddings\u003C/pre\u003E\n\u003Ch3\u003E3.2. Loading and Embedding Data\u003C/h3\u003E\n\u003Cp\u003EWe use a CSV file stored in the docs folder and load it using CSVLoader. The text in the CSV will be embedded with the Llama 3.2 model.\u003C/p\u003E\n\u003Cpre\u003Eparent_folder = os.getcwd() + \"/docs\"\u003Cbr\u003E\u003Cbr\u003E# Initialize the embedding model\u003Cbr\u003Eembeddings = OllamaEmbeddings(model=\"llama3.2\")\u003Cbr\u003E\u003Cbr\u003E# Load the CSV dataset\u003Cbr\u003Eloader = CSVLoader(parent_folder + \"/timeoff_datasets.csv\", encoding=\"windows-1252\")\u003Cbr\u003Edocuments = loader.load()\u003C/pre\u003E\n\u003Ch3\u003E3.3. Creating a Vector Database with ChromaDB\u003C/h3\u003E\n\u003Cp\u003EWe store the loaded documents in a Chroma vector database. This allows for efficient information retrieval based on the similarity of embedded content.\u003C/p\u003E\n\u003Cpre\u003E# Create a local ChromaDB instance with the embedded documents\u003Cbr\u003Edb = Chroma.from_documents(documents, embeddings)\u003Cbr\u003E\u003Cbr\u003E# Convert the database into a retriever\u003Cbr\u003Eretriever = db.as_retriever()\u003C/pre\u003E\n\u003Ch3\u003E3.4. Designing a Prompt Template\u003C/h3\u003E\n\u003Cp\u003ENext, we define a prompt template that structures the query to ensure relevant answers are generated by the LLM based on the retrieved data.\u003C/p\u003E\n\u003Cpre\u003Etemplate = \"\"\"Answer the question based only on the following context:\u003Cbr\u003E{context}\u003Cbr\u003E\u003Cbr\u003EQuestion: {question}\u003Cbr\u003E\"\"\"\u003Cbr\u003E\u003Cbr\u003E# Use the template to build a prompt\u003Cbr\u003Eprompt = ChatPromptTemplate.from_template(template)\u003C/pre\u003E\n\u003Ch3\u003E3.5. Running the LangChain Processing Pipeline\u003C/h3\u003E\n\u003Cp\u003EWe now define the LangChain pipeline. It connects the components in sequence: retrieving relevant data, applying the prompt, invoking the Llama model, and parsing the output.\u003C/p\u003E\n\u003Cpre\u003E# Define the data processing chain\u003Cbr\u003Echain = (\u003Cbr\u003E    {\"context\": retriever, \"question\": RunnablePassthrough()}\u003Cbr\u003E    | prompt\u003Cbr\u003E    | model\u003Cbr\u003E    | StrOutputParser()\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003E# Example user query\u003Cbr\u003Euser_input = \"\"\"\u003Cbr\u003EPlease provide insights on the most frequent leave dates, patterns by weekdays, and suggestions for policy improvements.\u003Cbr\u003E\"\"\"\u003Cbr\u003E\u003Cbr\u003E# Execute the pipeline\u003Cbr\u003Eprint(chain.invoke(user_input))\u003C/pre\u003E\n\u003Ch3\u003E4. Use Case Applications\u003C/h3\u003E\n\u003Cp\u003EThe above code offers a flexible framework that can be adapted to several domains:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ECustomer Support Systems\u003C/strong\u003E: Retrieve FAQs or troubleshooting steps based on user queries.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EBusiness Intelligence\u003C/strong\u003E: Analyze and summarize trends or patterns from large datasets.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EEducation and Training\u003C/strong\u003E: Generate quiz answers or course summaries from uploaded materials.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EHealthcare\u003C/strong\u003E: Retrieve medical guidelines based on specific symptoms or conditions.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003EThis modular approach simplifies the integration of custom datasets and LLMs, making it suitable for dynamic applications across industries.\u003C/p\u003E\n\u003Ch3\u003E5. Challenges and Best Practices\u003C/h3\u003E\n\u003Cp\u003EWhile building data-driven applications, there are several challenges to consider:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EData Quality\u003C/strong\u003E: Ensure the datasets are well-structured to improve retrieval accuracy.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EModel Fine-tuning\u003C/strong\u003E: Use a suitable model (e.g., Llama 3.2) that aligns with the complexity of queries.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EPerformance Optimization\u003C/strong\u003E: Store embeddings locally to reduce latency during data retrieval.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EScalability\u003C/strong\u003E: For larger datasets, consider distributed storage solutions like FAISS along with Chroma.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E6. Conclusion\u003C/h3\u003E\n\u003Cp\u003EThis article demonstrates how to create a scalable, data-driven pipeline using LangChain, ChromaDB, and Llama models. The modular design allows easy customization and supports various real-world applications by efficiently combining document retrieval with LLM capabilities. With minimal code, developers can build powerful systems that respond intelligently to user queries, leveraging insights from both structured and unstructured data.\u003C/p\u003E\n\u003Cp\u003EThis framework lays the foundation for developing advanced applications across domains, empowering businesses with fast and effective decision-making tools.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f75f2f77888f\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Ch3\u003ETable of Contents\u003C/h3\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003EIntroduction\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ETechnologies Overview\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EStep-by-Step Code Walkthrough\u003C/strong\u003E\u003Cbr\u003E3.1. Preparing the Environment\u003Cbr\u003E3.2. Loading and Embedding Data\u003Cbr\u003E3.3. Creating a Vector Database with ChromaDB\u003Cbr\u003E3.4. Designing a Prompt Template\u003Cbr\u003E3.5. Running the LangChain Processing Pipeline\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EUse Case Applications\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EChallenges and Best Practices\u003C/strong\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EConclusion\u003C/strong\u003E\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/720/0*GzFIxT6Svov-EKHe.png\"\u003E\u003Cfigcaption\u003ELangchain Logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch3\u003E1. Introduction\u003C/h3\u003E\n\u003Cp\u003EData-driven applications are becoming essential in various domains, from customer service to data analysis. By combining LangChain’s modular framework with a powerful local vector database like ChromaDB and leveraging state-of-the-art models like Llama 3.2, we can build a flexible solution that integrates data retrieval and large language models (LLMs).\u003C/p\u003E\n\u003Cp\u003EIn this article, we’ll walk through how to build a pipeline that processes CSV data, creates embeddings, and answers queries efficiently using the LangChain framework.\u003C/p\u003E\n\u003Ch3\u003E2. Technologies Overview\u003C/h3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ELangChain\u003C/strong\u003E: A versatile library for building applications that involve LLMs. It allows you to chain together different components like prompts, models, and outputs.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EChromaDB\u003C/strong\u003E: A local vector storage engine used to store and retrieve documents based on their embeddings.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ELlama 3.2\u003C/strong\u003E: A private language model designed for complex reasoning and text-based tasks.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ECSVLoader\u003C/strong\u003E: A utility to load CSV datasets directly into LangChain workflows.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EEmbeddings\u003C/strong\u003E: A numerical representation of text used to measure similarity between documents or queries.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E3. Step-by-Step Code Walkthrough\u003C/h3\u003E\n\u003Ch3\u003E3.1. Preparing the Environment\u003C/h3\u003E\n\u003Cp\u003EFirst, we import the necessary libraries. This setup will enable us to load datasets, manage embeddings, and create a seamless data pipeline.\u003C/p\u003E\n\u003Cpre\u003Efrom langchain.vectorstores import Chroma\u003Cbr\u003Efrom langchain.prompts import ChatPromptTemplate\u003Cbr\u003Efrom langchain_core.output_parsers import StrOutputParser\u003Cbr\u003Efrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\u003Cbr\u003Efrom langchain_ollama.llms import OllamaLLM\u003Cbr\u003Efrom langchain_community.document_loaders.csv_loader import CSVLoader\u003Cbr\u003Eimport os\u003Cbr\u003Efrom pathlib import Path\u003Cbr\u003Efrom langchain_ollama import OllamaEmbeddings\u003C/pre\u003E\n\u003Ch3\u003E3.2. Loading and Embedding Data\u003C/h3\u003E\n\u003Cp\u003EWe use a CSV file stored in the docs folder and load it using CSVLoader. The text in the CSV will be embedded with the Llama 3.2 model.\u003C/p\u003E\n\u003Cpre\u003Eparent_folder = os.getcwd() + \"/docs\"\u003Cbr\u003E\u003Cbr\u003E# Initialize the embedding model\u003Cbr\u003Eembeddings = OllamaEmbeddings(model=\"llama3.2\")\u003Cbr\u003E\u003Cbr\u003E# Load the CSV dataset\u003Cbr\u003Eloader = CSVLoader(parent_folder + \"/timeoff_datasets.csv\", encoding=\"windows-1252\")\u003Cbr\u003Edocuments = loader.load()\u003C/pre\u003E\n\u003Ch3\u003E3.3. Creating a Vector Database with ChromaDB\u003C/h3\u003E\n\u003Cp\u003EWe store the loaded documents in a Chroma vector database. This allows for efficient information retrieval based on the similarity of embedded content.\u003C/p\u003E\n\u003Cpre\u003E# Create a local ChromaDB instance with the embedded documents\u003Cbr\u003Edb = Chroma.from_documents(documents, embeddings)\u003Cbr\u003E\u003Cbr\u003E# Convert the database into a retriever\u003Cbr\u003Eretriever = db.as_retriever()\u003C/pre\u003E\n\u003Ch3\u003E3.4. Designing a Prompt Template\u003C/h3\u003E\n\u003Cp\u003ENext, we define a prompt template that structures the query to ensure relevant answers are generated by the LLM based on the retrieved data.\u003C/p\u003E\n\u003Cpre\u003Etemplate = \"\"\"Answer the question based only on the following context:\u003Cbr\u003E{context}\u003Cbr\u003E\u003Cbr\u003EQuestion: {question}\u003Cbr\u003E\"\"\"\u003Cbr\u003E\u003Cbr\u003E# Use the template to build a prompt\u003Cbr\u003Eprompt = ChatPromptTemplate.from_template(template)\u003C/pre\u003E\n\u003Ch3\u003E3.5. Running the LangChain Processing Pipeline\u003C/h3\u003E\n\u003Cp\u003EWe now define the LangChain pipeline. It connects the components in sequence: retrieving relevant data, applying the prompt, invoking the Llama model, and parsing the output.\u003C/p\u003E\n\u003Cpre\u003E# Define the data processing chain\u003Cbr\u003Echain = (\u003Cbr\u003E    {\"context\": retriever, \"question\": RunnablePassthrough()}\u003Cbr\u003E    | prompt\u003Cbr\u003E    | model\u003Cbr\u003E    | StrOutputParser()\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003E# Example user query\u003Cbr\u003Euser_input = \"\"\"\u003Cbr\u003EPlease provide insights on the most frequent leave dates, patterns by weekdays, and suggestions for policy improvements.\u003Cbr\u003E\"\"\"\u003Cbr\u003E\u003Cbr\u003E# Execute the pipeline\u003Cbr\u003Eprint(chain.invoke(user_input))\u003C/pre\u003E\n\u003Ch3\u003E4. Use Case Applications\u003C/h3\u003E\n\u003Cp\u003EThe above code offers a flexible framework that can be adapted to several domains:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003E\n\u003Cstrong\u003ECustomer Support Systems\u003C/strong\u003E: Retrieve FAQs or troubleshooting steps based on user queries.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EBusiness Intelligence\u003C/strong\u003E: Analyze and summarize trends or patterns from large datasets.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EEducation and Training\u003C/strong\u003E: Generate quiz answers or course summaries from uploaded materials.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EHealthcare\u003C/strong\u003E: Retrieve medical guidelines based on specific symptoms or conditions.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003EThis modular approach simplifies the integration of custom datasets and LLMs, making it suitable for dynamic applications across industries.\u003C/p\u003E\n\u003Ch3\u003E5. Challenges and Best Practices\u003C/h3\u003E\n\u003Cp\u003EWhile building data-driven applications, there are several challenges to consider:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EData Quality\u003C/strong\u003E: Ensure the datasets are well-structured to improve retrieval accuracy.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EModel Fine-tuning\u003C/strong\u003E: Use a suitable model (e.g., Llama 3.2) that aligns with the complexity of queries.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EPerformance Optimization\u003C/strong\u003E: Store embeddings locally to reduce latency during data retrieval.\u003C/li\u003E\n\u003Cli\u003E\n\u003Cstrong\u003EScalability\u003C/strong\u003E: For larger datasets, consider distributed storage solutions like FAISS along with Chroma.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E6. Conclusion\u003C/h3\u003E\n\u003Cp\u003EThis article demonstrates how to create a scalable, data-driven pipeline using LangChain, ChromaDB, and Llama models. The modular design allows easy customization and supports various real-world applications by efficiently combining document retrieval with LLM capabilities. With minimal code, developers can build powerful systems that respond intelligently to user queries, leveraging insights from both structured and unstructured data.\u003C/p\u003E\n\u003Cp\u003EThis framework lays the foundation for developing advanced applications across domains, empowering businesses with fast and effective decision-making tools.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f75f2f77888f\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "llm",
          "python",
          "ai",
          "data-engineering"
        ]
      },
      {
        "title": "Building Scalable Microservices with gRPC in Golang: A Step-by-Step Guide",
        "pubDate": "2024-08-22 01:24:38",
        "link": "https://nasriadzlani.medium.com/building-scalable-microservices-with-grpc-in-golang-a-step-by-step-guide-5d0aab2cd819?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/5d0aab2cd819",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EgRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/0*wB-UbjtmcZTrP13n.jpg\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EWhy Golang and gRPC?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EGolang’s simplicity, concurrency, and static typing, combined with gRPC’s efficient binary serialization, make it a powerful combination for building scalable and maintainable microservices. The strong typing provided by Protocol Buffers ensures consistency and reduces errors, while Golang’s tooling and the active community support make development a pleasure.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EWhat is Protocol Buffers?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EProtocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data — think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.\u003C/p\u003E\n\u003Cp\u003EExample:\u003C/p\u003E\n\u003Cpre\u003Esyntax = \"proto3\";\u003Cbr\u003E\u003Cbr\u003Eoption go_package = \"grpc-test/proto/user\";\u003Cbr\u003E\u003Cbr\u003Eservice DataUser {\u003Cbr\u003E  rpc GetUser (GetUserRequest) returns (GetUserResponse) {}\u003Cbr\u003E  rpc ListUser (ListUserRequest) returns (ListUserResponse) {}\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage GetUserRequest {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage GetUserResponse {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E  string email = 3;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage ListUserRequest {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage ListUserResponse {\u003Cbr\u003E  repeated GetUserResponse users = 1;\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Ecode at proto is like struct and interface repository.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECompile a proto file into golang file\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003Ei have protofile in proto folder at project directory with ‘user.proto’ filename\u003C/p\u003E\n\u003Cpre\u003Eprotoc --go_out=. --go_opt=paths=source_relative \\\u003Cbr\u003E    --go-grpc_out=. --go-grpc_opt=paths=source_relative \\\u003Cbr\u003E    proto/user.proto\u003C/pre\u003E\n\u003Cp\u003Eafter compiling successfully, compiler will be generate 2 golang file user_grpc.pb.go and user.pb.go\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EHow to make a gRPC server\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003Ecreate a file main.go at project directory for server\u003C/p\u003E\n\u003Cpre\u003Epackage main\u003Cbr\u003E\u003Cbr\u003Eimport (\u003Cbr\u003E \"flag\"\u003Cbr\u003E \"fmt\"\u003Cbr\u003E pb \"grpc-test/proto\"\u003Cbr\u003E \"grpc-test/service\"\u003Cbr\u003E \"log\"\u003Cbr\u003E \"net\"\u003Cbr\u003E\u003Cbr\u003E \"google.golang.org/grpc\"\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Evar (\u003Cbr\u003E port = flag.Int(\"port\", 50051, \"The server port\")\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Efunc main() {\u003Cbr\u003E flag.Parse()\u003Cbr\u003E lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))\u003Cbr\u003E if err != nil {\u003Cbr\u003E  log.Fatalf(\"failed to listen: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E s := grpc.NewServer()\u003Cbr\u003E pb.RegisterDataUserServer(s, &amp;service.Server{})\u003Cbr\u003E log.Printf(\"server listening at %v\", lis.Addr())\u003Cbr\u003E if err := s.Serve(lis); err != nil {\u003Cbr\u003E  log.Fatalf(\"failed to serve: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Eand i have specific folder and file for services ‘service/user.service.go’\u003C/p\u003E\n\u003Cpre\u003Epackage service\u003Cbr\u003E\u003Cbr\u003Eimport (\u003Cbr\u003E \"context\"\u003Cbr\u003E \"fmt\"\u003Cbr\u003E pb \"grpc-test/proto\"\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Etype User struct {\u003Cbr\u003E Id    int32\u003Cbr\u003E Name  string\u003Cbr\u003E Email string\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Etype Server struct {\u003Cbr\u003E pb.UnimplementedDataUserServer\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Efunc (s *Server) GetUser(ctx context.Context, in *pb.GetUserRequest) (*pb.GetUserResponse, error) {\u003Cbr\u003E fmt.Println(\"hello \", in.Id)\u003Cbr\u003E return &amp;pb.GetUserResponse{Id: in.Id, Name: \"test\", Email: \"testing@mail.com\"}, nil\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Efunc (s *Server) ListUser(ctx context.Context, in *pb.ListUserRequest) (*pb.ListUserResponse, error) {\u003Cbr\u003E return &amp;pb.ListUserResponse{Users: []*pb.GetUserResponse{\u003Cbr\u003E  {Id: \"1\", Name: \"test\", Email: \"HEHE@mail.com\"},\u003Cbr\u003E  {Id: \"2\", Name: \"test2\", Email: \"wwkwkwk@mail.com\"},\u003Cbr\u003E }}, nil\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Eand this is a my folder structure\u003C/p\u003E\n\u003Cpre\u003E├── go.mod\u003Cbr\u003E├── go.sum\u003Cbr\u003E├── main.go\u003Cbr\u003E├── proto\u003Cbr\u003E│   ├── user.pb.go\u003Cbr\u003E│   ├── user.proto\u003Cbr\u003E│   └── user_grpc.pb.go\u003Cbr\u003E├── service\u003Cbr\u003E│   ├── user.service.go\u003Cbr\u003E│   └── user_test.service.go\u003Cbr\u003E└── tmp\u003Cbr\u003E    └── main\u003C/pre\u003E\n\u003Cp\u003E\u003Cstrong\u003ENow, Create a Client\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ECreate a project directory for a client and create golang file like a main\u003C/p\u003E\n\u003Cp\u003EConnect to a grpc server like this\u003C/p\u003E\n\u003Cpre\u003Eflag.Parse()\u003Cbr\u003E // Set up a connection to the server.\u003Cbr\u003E addr := flag.String(\"addr\", \"localhost:50051\", \"the address to connect to\")\u003Cbr\u003E conn, err := grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()))\u003Cbr\u003E if err != nil {\u003Cbr\u003E  log.Fatalf(\"did not connect: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E defer conn.Close()\u003C/pre\u003E\n\u003Cp\u003EChange a port and matching to your grpc server port\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d0aab2cd819\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EgRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/0*wB-UbjtmcZTrP13n.jpg\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EWhy Golang and gRPC?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EGolang’s simplicity, concurrency, and static typing, combined with gRPC’s efficient binary serialization, make it a powerful combination for building scalable and maintainable microservices. The strong typing provided by Protocol Buffers ensures consistency and reduces errors, while Golang’s tooling and the active community support make development a pleasure.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EWhat is Protocol Buffers?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EProtocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data — think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.\u003C/p\u003E\n\u003Cp\u003EExample:\u003C/p\u003E\n\u003Cpre\u003Esyntax = \"proto3\";\u003Cbr\u003E\u003Cbr\u003Eoption go_package = \"grpc-test/proto/user\";\u003Cbr\u003E\u003Cbr\u003Eservice DataUser {\u003Cbr\u003E  rpc GetUser (GetUserRequest) returns (GetUserResponse) {}\u003Cbr\u003E  rpc ListUser (ListUserRequest) returns (ListUserResponse) {}\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage GetUserRequest {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage GetUserResponse {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E  string email = 3;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage ListUserRequest {\u003Cbr\u003E  string id = 1;\u003Cbr\u003E  string name = 2;\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Emessage ListUserResponse {\u003Cbr\u003E  repeated GetUserResponse users = 1;\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Ecode at proto is like struct and interface repository.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECompile a proto file into golang file\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003Ei have protofile in proto folder at project directory with ‘user.proto’ filename\u003C/p\u003E\n\u003Cpre\u003Eprotoc --go_out=. --go_opt=paths=source_relative \\\u003Cbr\u003E    --go-grpc_out=. --go-grpc_opt=paths=source_relative \\\u003Cbr\u003E    proto/user.proto\u003C/pre\u003E\n\u003Cp\u003Eafter compiling successfully, compiler will be generate 2 golang file user_grpc.pb.go and user.pb.go\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EHow to make a gRPC server\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003Ecreate a file main.go at project directory for server\u003C/p\u003E\n\u003Cpre\u003Epackage main\u003Cbr\u003E\u003Cbr\u003Eimport (\u003Cbr\u003E \"flag\"\u003Cbr\u003E \"fmt\"\u003Cbr\u003E pb \"grpc-test/proto\"\u003Cbr\u003E \"grpc-test/service\"\u003Cbr\u003E \"log\"\u003Cbr\u003E \"net\"\u003Cbr\u003E\u003Cbr\u003E \"google.golang.org/grpc\"\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Evar (\u003Cbr\u003E port = flag.Int(\"port\", 50051, \"The server port\")\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Efunc main() {\u003Cbr\u003E flag.Parse()\u003Cbr\u003E lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))\u003Cbr\u003E if err != nil {\u003Cbr\u003E  log.Fatalf(\"failed to listen: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E s := grpc.NewServer()\u003Cbr\u003E pb.RegisterDataUserServer(s, &amp;service.Server{})\u003Cbr\u003E log.Printf(\"server listening at %v\", lis.Addr())\u003Cbr\u003E if err := s.Serve(lis); err != nil {\u003Cbr\u003E  log.Fatalf(\"failed to serve: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Eand i have specific folder and file for services ‘service/user.service.go’\u003C/p\u003E\n\u003Cpre\u003Epackage service\u003Cbr\u003E\u003Cbr\u003Eimport (\u003Cbr\u003E \"context\"\u003Cbr\u003E \"fmt\"\u003Cbr\u003E pb \"grpc-test/proto\"\u003Cbr\u003E)\u003Cbr\u003E\u003Cbr\u003Etype User struct {\u003Cbr\u003E Id    int32\u003Cbr\u003E Name  string\u003Cbr\u003E Email string\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Etype Server struct {\u003Cbr\u003E pb.UnimplementedDataUserServer\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Efunc (s *Server) GetUser(ctx context.Context, in *pb.GetUserRequest) (*pb.GetUserResponse, error) {\u003Cbr\u003E fmt.Println(\"hello \", in.Id)\u003Cbr\u003E return &amp;pb.GetUserResponse{Id: in.Id, Name: \"test\", Email: \"testing@mail.com\"}, nil\u003Cbr\u003E}\u003Cbr\u003E\u003Cbr\u003Efunc (s *Server) ListUser(ctx context.Context, in *pb.ListUserRequest) (*pb.ListUserResponse, error) {\u003Cbr\u003E return &amp;pb.ListUserResponse{Users: []*pb.GetUserResponse{\u003Cbr\u003E  {Id: \"1\", Name: \"test\", Email: \"HEHE@mail.com\"},\u003Cbr\u003E  {Id: \"2\", Name: \"test2\", Email: \"wwkwkwk@mail.com\"},\u003Cbr\u003E }}, nil\u003Cbr\u003E}\u003C/pre\u003E\n\u003Cp\u003Eand this is a my folder structure\u003C/p\u003E\n\u003Cpre\u003E├── go.mod\u003Cbr\u003E├── go.sum\u003Cbr\u003E├── main.go\u003Cbr\u003E├── proto\u003Cbr\u003E│   ├── user.pb.go\u003Cbr\u003E│   ├── user.proto\u003Cbr\u003E│   └── user_grpc.pb.go\u003Cbr\u003E├── service\u003Cbr\u003E│   ├── user.service.go\u003Cbr\u003E│   └── user_test.service.go\u003Cbr\u003E└── tmp\u003Cbr\u003E    └── main\u003C/pre\u003E\n\u003Cp\u003E\u003Cstrong\u003ENow, Create a Client\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ECreate a project directory for a client and create golang file like a main\u003C/p\u003E\n\u003Cp\u003EConnect to a grpc server like this\u003C/p\u003E\n\u003Cpre\u003Eflag.Parse()\u003Cbr\u003E // Set up a connection to the server.\u003Cbr\u003E addr := flag.String(\"addr\", \"localhost:50051\", \"the address to connect to\")\u003Cbr\u003E conn, err := grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()))\u003Cbr\u003E if err != nil {\u003Cbr\u003E  log.Fatalf(\"did not connect: %v\", err)\u003Cbr\u003E }\u003Cbr\u003E defer conn.Close()\u003C/pre\u003E\n\u003Cp\u003EChange a port and matching to your grpc server port\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d0aab2cd819\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "grpc",
          "golang-grpc",
          "golang"
        ]
      },
      {
        "title": "Free serverless for restAPI with supabase",
        "pubDate": "2024-07-28 12:04:04",
        "link": "https://nasriadzlani.medium.com/free-serverless-for-restapi-with-supabase-906021f294c5?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/906021f294c5",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003ESupabase is an open-source Firebase alternative that is rapidly gaining popularity in the developer community. supabase has many features like Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, Storage, and Vector embeddings.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*KyMeuRwaprE-47HY.png\"\u003E\u003Cfigcaption\u003ESupabase Logo. src \u003Ca href=\"https://www.squarepeg.vc/companies/supabase\"\u003Ehttps://www.squarepeg.vc/companies/supabase\u003C/a\u003E\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003EThis article is a summary of my experience learning about Supabase for my research. First, I created a Supabase account at \u003Ca href=\"https://supabase.com/dashboard/sign-in\"\u003ESupabase Dashboard\u003C/a\u003E. You can sign up using your email and password, GitHub account, or other SSO options. After completing the registration details, you can create a project in the Supabase dashboard.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*057nvF1TFMqOZcAyPnp4lQ.png\"\u003E\u003C/figure\u003E\u003Cp\u003EOnce you’ve created your project, go to Supabase Dashboard. The first thing you’ll need to do is install the Supabase CLI on your local machine. For reference, I’m using a Mac.\u003C/p\u003E\n\u003Cpre\u003Ebrew install supabase/tap/supabase\u003C/pre\u003E\n\u003Cp\u003EFor more details, you can check \u003Ca href=\"https://supabase.com/docs/guides/cli/getting-started\"\u003ESupabase CLI Getting Started Guide\u003C/a\u003E. After installing the CLI, you can log in to Supabase directly from your terminal.\u003C/p\u003E\n\u003Cpre\u003Esupabase login\u003C/pre\u003E\n\u003Cp\u003ESupabase will redirect you to your browser to log in and sync your account from the browser to your local machine. After that, you can create a function project like this:\u003C/p\u003E\n\u003Cpre\u003Esupabase functions new &lt;function-name&gt;\u003C/pre\u003E\n\u003Cp\u003EWhen you open the project, it will look like this:\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UJE9IbNUVghJYl6fc3Iyfw.png\"\u003E\u003C/figure\u003E\u003Cp\u003EAfter some searching, I found that I needed to install the Deno extension in VSCode.\u003C/p\u003E\n\u003Cp\u003EDeployment:\u003C/p\u003E\n\u003Cp\u003EDeploying this serverless function is quite simple. Just run the following command:\u003C/p\u003E\n\u003Cpre\u003Esupabase functions deploy pilkada-mobile-svc --project-ref &lt;id&gt;\u003C/pre\u003E\n\u003Cp\u003EWhen the deployment is successful, your terminal will show something like this:\u003C/p\u003E\n\u003Cpre\u003E\u003Cbr\u003EBundling Function: pilkada-mobile-svc\u003Cbr\u003EDownload https://jsr.io/@supabase/functions-js/meta.json\u003Cbr\u003EDownload https://jsr.io/@supabase/functions-js/2.4.2_meta.json\u003Cbr\u003EDeploying Function: pilkada-mobile-svc (script size: 2.261MB)\u003Cbr\u003EDeployed Functions on project &lt;id&gt;: pilkada-mobile-svc\u003Cbr\u003EYou can inspect your deployment in the Dashboard: https://supabase.com/dashboard/project/&lt;id&gt;/functions\u003C/pre\u003E\n\u003Cp\u003ETesting:\u003C/p\u003E\n\u003Cp\u003EI’m using HTTPie to test my service. It looks like this:\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5nIAfha1RrvWXDfTvI95pw.png\"\u003E\u003C/figure\u003E\u003Cp\u003EConclusion\u003C/p\u003E\n\u003Cp\u003ELearning Supabase has been an insightful journey. From setting up an account and creating projects to deploying serverless functions, the process is straightforward and user-friendly. The documentation and tools provided by Supabase, like the CLI and integration with Deno in VSCode, make it easy to get started and deploy your services efficiently.\u003C/p\u003E\n\u003Cp\u003EFor testing, HTTPie proved to be a valuable tool, allowing me to quickly and easily verify my endpoints. Whether you’re a beginner or an experienced developer, Supabase offers a robust platform for building and deploying applications.\u003C/p\u003E\n\u003Cp\u003EFeel free to share your experiences or any tips you have in the comments. Happy coding!.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=906021f294c5\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003ESupabase is an open-source Firebase alternative that is rapidly gaining popularity in the developer community. supabase has many features like Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, Storage, and Vector embeddings.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*KyMeuRwaprE-47HY.png\"\u003E\u003Cfigcaption\u003ESupabase Logo. src \u003Ca href=\"https://www.squarepeg.vc/companies/supabase\"\u003Ehttps://www.squarepeg.vc/companies/supabase\u003C/a\u003E\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003EThis article is a summary of my experience learning about Supabase for my research. First, I created a Supabase account at \u003Ca href=\"https://supabase.com/dashboard/sign-in\"\u003ESupabase Dashboard\u003C/a\u003E. You can sign up using your email and password, GitHub account, or other SSO options. After completing the registration details, you can create a project in the Supabase dashboard.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*057nvF1TFMqOZcAyPnp4lQ.png\"\u003E\u003C/figure\u003E\u003Cp\u003EOnce you’ve created your project, go to Supabase Dashboard. The first thing you’ll need to do is install the Supabase CLI on your local machine. For reference, I’m using a Mac.\u003C/p\u003E\n\u003Cpre\u003Ebrew install supabase/tap/supabase\u003C/pre\u003E\n\u003Cp\u003EFor more details, you can check \u003Ca href=\"https://supabase.com/docs/guides/cli/getting-started\"\u003ESupabase CLI Getting Started Guide\u003C/a\u003E. After installing the CLI, you can log in to Supabase directly from your terminal.\u003C/p\u003E\n\u003Cpre\u003Esupabase login\u003C/pre\u003E\n\u003Cp\u003ESupabase will redirect you to your browser to log in and sync your account from the browser to your local machine. After that, you can create a function project like this:\u003C/p\u003E\n\u003Cpre\u003Esupabase functions new &lt;function-name&gt;\u003C/pre\u003E\n\u003Cp\u003EWhen you open the project, it will look like this:\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UJE9IbNUVghJYl6fc3Iyfw.png\"\u003E\u003C/figure\u003E\u003Cp\u003EAfter some searching, I found that I needed to install the Deno extension in VSCode.\u003C/p\u003E\n\u003Cp\u003EDeployment:\u003C/p\u003E\n\u003Cp\u003EDeploying this serverless function is quite simple. Just run the following command:\u003C/p\u003E\n\u003Cpre\u003Esupabase functions deploy pilkada-mobile-svc --project-ref &lt;id&gt;\u003C/pre\u003E\n\u003Cp\u003EWhen the deployment is successful, your terminal will show something like this:\u003C/p\u003E\n\u003Cpre\u003E\u003Cbr\u003EBundling Function: pilkada-mobile-svc\u003Cbr\u003EDownload https://jsr.io/@supabase/functions-js/meta.json\u003Cbr\u003EDownload https://jsr.io/@supabase/functions-js/2.4.2_meta.json\u003Cbr\u003EDeploying Function: pilkada-mobile-svc (script size: 2.261MB)\u003Cbr\u003EDeployed Functions on project &lt;id&gt;: pilkada-mobile-svc\u003Cbr\u003EYou can inspect your deployment in the Dashboard: https://supabase.com/dashboard/project/&lt;id&gt;/functions\u003C/pre\u003E\n\u003Cp\u003ETesting:\u003C/p\u003E\n\u003Cp\u003EI’m using HTTPie to test my service. It looks like this:\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5nIAfha1RrvWXDfTvI95pw.png\"\u003E\u003C/figure\u003E\u003Cp\u003EConclusion\u003C/p\u003E\n\u003Cp\u003ELearning Supabase has been an insightful journey. From setting up an account and creating projects to deploying serverless functions, the process is straightforward and user-friendly. The documentation and tools provided by Supabase, like the CLI and integration with Deno in VSCode, make it easy to get started and deploy your services efficiently.\u003C/p\u003E\n\u003Cp\u003EFor testing, HTTPie proved to be a valuable tool, allowing me to quickly and easily verify my endpoints. Whether you’re a beginner or an experienced developer, Supabase offers a robust platform for building and deploying applications.\u003C/p\u003E\n\u003Cp\u003EFeel free to share your experiences or any tips you have in the comments. Happy coding!.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=906021f294c5\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "typescript",
          "programming",
          "coding",
          "serverless",
          "supabase"
        ]
      },
      {
        "title": "Perbandingan Loop dan Vectorization",
        "pubDate": "2024-01-13 09:43:25",
        "link": "https://nasriadzlani.medium.com/perbandingan-loop-dan-vectorization-56adfafe5d66?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/56adfafe5d66",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*F3nC-B0Mt375waeE.png\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EPendahuluan\u003C/strong\u003E\u003Cbr\u003EPengulangan datang secara alami kepada kita, kita belajar tentang pengulangan di hampir semua bahasa pemrograman. Jadi, secara default, kita mulai menerapkan pengulangan setiap kali ada operasi yang repetitif. Tetapi ketika kita bekerja dengan jumlah iterasi yang besar (jutaan/miliaran baris), menggunakan pengulangan bisa dianggap sebagai tindakan yang kurang efisien. Mungkin Anda akan terjebak untuk beberapa jam, baru kemudian menyadari bahwa itu tidak akan berhasil. Inilah saatnya menerapkan Vectorization dalam Python menjadi sangat penting.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EApa itu Vectorization?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EVectorization adalah teknik menerapkan operasi array (NumPy) pada sebuah dataset. Secara latar belakang, ia menerapkan operasi pada semua elemen array atau rangkaian sekaligus (berbeda dengan pengulangan ‘for’ yang memanipulasi satu baris pada satu waktu).\u003C/p\u003E\n\u003Cp\u003EDalam artikel ini, kita akan melihat beberapa kasus penggunaan di mana kita dapat dengan mudah menggantikan pengulangan Python dengan Vectorization. Hal ini akan membantu Anda menghemat waktu dan menjadi lebih terampil dalam pemrograman.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 1: Menemukan Jumlah Bilangan\u003C/strong\u003E\u003Cbr\u003EPertama, kita akan melihat contoh dasar dalam menemukan jumlah bilangan menggunakan pengulangan dan Vectorization dalam Python.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time\u003Cbr\u003Estart = time.time()\u003Cbr\u003E# jumlah secara iteratif\u003Cbr\u003Etotal = 0\u003Cbr\u003E# mengulang sebanyak 1,5 Juta angka\u003Cbr\u003Efor item in range(0, 1500000):\u003Cbr\u003E  total = total + item\u003Cbr\u003Eprint('Jumlahnya adalah:' + str(total))\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E# 1124999250000\u003Cbr\u003E# 0,14 Detikp\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Estart = time.time()\u003Cbr\u003E# jumlah vektor - menggunakan numpy untuk vectorization\u003Cbr\u003E# np.arange membuat urutan angka dari 0 hingga 1499999\u003Cbr\u003Eprint(np.sum(np.arange(1500000)))\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E# 1124999250000\u003Cbr\u003E# 0,008 Detik\u003C/pre\u003E\n\u003Cp\u003EVectorization membutuhkan waktu ~18x lebih singkat untuk dieksekusi dibandingkan dengan iterasi menggunakan fungsi range. Perbedaan ini akan menjadi lebih signifikan saat bekerja dengan DataFrame Pandas.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 2: Operasi Matematika (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDalam Ilmu Data, saat bekerja dengan Pandas DataFrame, para pengembang menggunakan pengulangan untuk membuat kolom turunan baru menggunakan operasi matematika.\u003C/p\u003E\n\u003Cp\u003EPada contoh berikut, kita dapat melihat seberapa mudahnya pengulangan dapat digantikan dengan Vectorization untuk kasus penggunaan seperti ini.\u003C/p\u003E\n\u003Cp\u003EMembuat DataFrame\u003C/p\u003E\n\u003Cp\u003EDataFrame adalah data tabular dalam bentuk baris dan kolom.\u003C/p\u003E\n\u003Cp\u003EKami membuat DataFrame Pandas dengan 5 Juta baris dan 4 kolom yang diisi dengan nilai acak antara 0 dan 50.\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Eimport pandas as pd\u003Cbr\u003Edf = pd.DataFrame(np.random.randint(0, 50, size=(5000000, 4)), columns=('a', 'b', 'c', 'd'))\u003Cbr\u003Edf.shape\u003Cbr\u003E# (5000000, 5)\u003Cbr\u003Edf.head()\u003C/pre\u003E\n\u003Cp\u003EKami akan membuat kolom baru ‘ratio’ untuk menemukan rasio dari kolom ‘d’ dan ‘c’.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time \u003Cbr\u003Estart = time.time()\u003Cbr\u003E# Mengulang melalui DataFrame menggunakan iterrows\u003Cbr\u003Efor idx, row in df.iterrows():\u003Cbr\u003E # membuat kolom baru \u003Cbr\u003E df.at[idx, 'ratio'] = 100 * (row[\"d\"] / row[\"c\"]) \u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### 109 Detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Estart = time.time()\u003Cbr\u003Edf[\"ratio\"] = 100 * (df[\"d\"] / df[\"c\"])\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### 0,12 detik\u003C/pre\u003E\n\u003Cp\u003EKita dapat melihat peningkatan yang signifikan dengan DataFrame, waktu yang dibutuhkan oleh operasi Vectorization hampir 1000x lebih cepat dibandingkan dengan pengulangan di Python.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 3: Pernyataan If-else (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EKita mengimplementasikan banyak operasi yang memerlukan kita untuk menggunakan logika ‘If-else’. Kita dapat dengan mudah menggantikan logika ini dengan operasi Vectorization dalam Python.\u003C/p\u003E\n\u003Cp\u003EMari lihat contoh berikut untuk memahaminya lebih baik (kita akan menggunakan DataFrame yang telah kita buat pada kasus penggunaan 2):\u003C/p\u003E\n\u003Cp\u003EBayangkan kita ingin membuat kolom baru ‘e’ berdasarkan beberapa kondisi pada kolom yang ada ‘a’.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time \u003Cbr\u003Estart = time.time()\u003Cbr\u003E# Mengulang melalui DataFrame menggunakan iterrows\u003Cbr\u003Efor idx, row in df.iterrows():\u003Cbr\u003E if row.a == 0:\u003Cbr\u003E df.at[idx, 'e'] = row.d \u003Cbr\u003E elif (row.a &lt;= 25) &amp; (row.a &gt; 0):\u003Cbr\u003E df.at[idx, 'e'] = (row.b) - (row.c) \u003Cbr\u003E else:\u003Cbr\u003E df.at[idx, 'e'] = row.b + row.c\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### Waktu yang dibutuhkan: 177 detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003E# menggunakan vectorization \u003Cbr\u003Estart = time.time()\u003Cbr\u003Edf['e'] = df['b'] + df['c']\u003Cbr\u003Edf.loc[df['a'] &lt;= 25, 'e'] = df['b'] - df['c']\u003Cbr\u003Edf.loc[df['a'] == 0, 'e'] = df['d']\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E## 0,28007707595825195 detik\u003C/pre\u003E\n\u003Cp\u003EWaktu yang dibutuhkan oleh operasi Vectorization 600x lebih cepat dibandingkan dengan pengulangan Python dengan pernyataan if-else.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 4: Menyelesaikan Jaringan Pembelajaran Mesin/Deep Learning\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EPembelajaran Mendalam memerlukan kita untuk menyelesaikan beberapa persamaan kompleks dan itu pun untuk jutaan dan miliaran baris. Menjalankan pengulangan di Python untuk menyelesaikan persamaan-persamaan ini sangat lambat dan Vectorization adalah solusi optimal.\u003C/p\u003E\n\u003Cp\u003ESebagai contoh, untuk menghitung nilai y untuk jutaan baris dalam persamaan regresi multi-linier:\u003C/p\u003E\n\u003Cp\u003EPersamaan Regresi Linier (Gambar oleh Penulis)\u003C/p\u003E\n\u003Cp\u003EKita dapat menggantikan pengulangan dengan Vectorization.\u003C/p\u003E\n\u003Cp\u003ENilai-nilai m1, m2, m3,… ditentukan dengan menyelesaikan persamaan di atas menggunakan jutaan nilai yang sesuai dengan x1, x2, x3,… (untuk kesederhanaan, kita hanya akan melihat langkah perkalian sederhana)\u003C/p\u003E\n\u003Cp\u003EMembuat Data\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003E# menetapkan nilai awal m \u003Cbr\u003Em = np.random.rand(1,5)\u003Cbr\u003E# nilai input untuk 5 juta baris\u003Cbr\u003Ex = np.random.rand(5000000,5)\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Em = np.random.rand(1, 5)\u003Cbr\u003Ex = np.random.rand(5000000, 5)\u003Cbr\u003Etotal = 0\u003Cbr\u003Etic = time.process_time()\u003Cbr\u003Efor i in range(0, 5000000):\u003Cbr\u003E total = 0\u003Cbr\u003E for j in range(0, 5):\u003Cbr\u003E total = total + x[i][j] * m[0][j] \u003Cbr\u003E\u003Cbr\u003E zer[i] = total \u003Cbr\u003Etoc = time.process_time()\u003Cbr\u003Eprint(\"Waktu komputasi = \" + str((toc - tic)) + \"detik\")\u003Cbr\u003E####Waktu komputasi = 28,228 detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Etic = time.process_time()\u003Cbr\u003E# perkalian dot\u003Cbr\u003Enp.dot(x, m.T) \u003Cbr\u003Etoc = time.process_time()\u003Cbr\u003Eprint(\"Waktu komputasi = \" + str((toc - tic)) + \"detik\")\u003Cbr\u003E####Waktu komputasi = 0,107 detik\u003C/pre\u003E\n\u003Cp\u003E`np.dot` mengimplementasikan perkalian matriks Vectorized di belakang layar. Ini 165x lebih cepat dibandingkan dengan pengulangan di Python.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ERingkasan:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDalam tiga kasus penggunaan yang dibahas, kita melihat betapa efisiennya Vectorization dibandingkan dengan pengulangan dalam Python, terutama saat bekerja dengan DataFrame Pandas dan operasi matematika kompleks.\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003E\u003Cstrong\u003EPenggunaan 1: Menemukan Jumlah Bilangan\u003C/strong\u003E\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003EDengan menggunakan pengulangan, waktu eksekusi adalah 0,14 detik.\u003C/li\u003E\n\u003Cli\u003EDengan menggunakan Vectorization (NumPy), waktu eksekusi hanya 0,008 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization membutuhkan waktu ~18x lebih sedikit dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E2. Penggunaan 2: Operasi Matematika (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan memakan waktu 177 detik untuk membuat kolom ‘ratio’ pada DataFrame.\u003C/li\u003E\n\u003Cli\u003EVectorization hanya memerlukan waktu 0,28007707595825195 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization mempercepat operasi sekitar 600x dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E3. Penggunaan 3: Pernyataan If-else (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan membutuhkan waktu 177 detik untuk menentukan nilai kolom ‘e’ berdasarkan kondisi pada kolom ‘a’.\u003C/li\u003E\n\u003Cli\u003EVectorization hanya memerlukan waktu 0,28007707595825195 detik.\u003C/li\u003E\n\u003Cli\u003EWaktu eksekusi Vectorization sekitar 600x lebih cepat dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E4. Penggunaan 4 (Lanjutan): Menyelesaikan Jaringan Pembelajaran Mesin/Deep Learning.\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EDalam kasus ini, Vectorization sangat optimal untuk menyelesaikan persamaan kompleks dalam jutaan dan miliaran baris.\u003C/li\u003E\n\u003Cli\u003EMemanfaatkan NumPy dan operasi vektorisasi dapat memberikan peningkatan performa yang signifikan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E5. Penggunaan Tambahan: Matriks Dot Product\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan memakan waktu 28,228 detik untuk melakukan perkalian dot pada dua matriks.\u003C/li\u003E\n\u003Cli\u003EVectorization menggunakan `np.dot` hanya memerlukan waktu 0,107 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization mengungguli pengulangan sekitar 165x dalam hal kecepatan eksekusi.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EDalam semua kasus, penggunaan Vectorization, terutama dengan NumPy, dapat memberikan peningkatan kinerja yang luar biasa, terutama ketika bekerja dengan dataset yang besar.\u003C/p\u003E\n\u003Cp\u003Ereference : \u003Ca href=\"https://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e8b0172b9581\"\u003Ehttps://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e8b0172b9581\u003C/a\u003E\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=56adfafe5d66\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*F3nC-B0Mt375waeE.png\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EPendahuluan\u003C/strong\u003E\u003Cbr\u003EPengulangan datang secara alami kepada kita, kita belajar tentang pengulangan di hampir semua bahasa pemrograman. Jadi, secara default, kita mulai menerapkan pengulangan setiap kali ada operasi yang repetitif. Tetapi ketika kita bekerja dengan jumlah iterasi yang besar (jutaan/miliaran baris), menggunakan pengulangan bisa dianggap sebagai tindakan yang kurang efisien. Mungkin Anda akan terjebak untuk beberapa jam, baru kemudian menyadari bahwa itu tidak akan berhasil. Inilah saatnya menerapkan Vectorization dalam Python menjadi sangat penting.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EApa itu Vectorization?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EVectorization adalah teknik menerapkan operasi array (NumPy) pada sebuah dataset. Secara latar belakang, ia menerapkan operasi pada semua elemen array atau rangkaian sekaligus (berbeda dengan pengulangan ‘for’ yang memanipulasi satu baris pada satu waktu).\u003C/p\u003E\n\u003Cp\u003EDalam artikel ini, kita akan melihat beberapa kasus penggunaan di mana kita dapat dengan mudah menggantikan pengulangan Python dengan Vectorization. Hal ini akan membantu Anda menghemat waktu dan menjadi lebih terampil dalam pemrograman.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 1: Menemukan Jumlah Bilangan\u003C/strong\u003E\u003Cbr\u003EPertama, kita akan melihat contoh dasar dalam menemukan jumlah bilangan menggunakan pengulangan dan Vectorization dalam Python.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time\u003Cbr\u003Estart = time.time()\u003Cbr\u003E# jumlah secara iteratif\u003Cbr\u003Etotal = 0\u003Cbr\u003E# mengulang sebanyak 1,5 Juta angka\u003Cbr\u003Efor item in range(0, 1500000):\u003Cbr\u003E  total = total + item\u003Cbr\u003Eprint('Jumlahnya adalah:' + str(total))\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E# 1124999250000\u003Cbr\u003E# 0,14 Detikp\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Estart = time.time()\u003Cbr\u003E# jumlah vektor - menggunakan numpy untuk vectorization\u003Cbr\u003E# np.arange membuat urutan angka dari 0 hingga 1499999\u003Cbr\u003Eprint(np.sum(np.arange(1500000)))\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E# 1124999250000\u003Cbr\u003E# 0,008 Detik\u003C/pre\u003E\n\u003Cp\u003EVectorization membutuhkan waktu ~18x lebih singkat untuk dieksekusi dibandingkan dengan iterasi menggunakan fungsi range. Perbedaan ini akan menjadi lebih signifikan saat bekerja dengan DataFrame Pandas.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 2: Operasi Matematika (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDalam Ilmu Data, saat bekerja dengan Pandas DataFrame, para pengembang menggunakan pengulangan untuk membuat kolom turunan baru menggunakan operasi matematika.\u003C/p\u003E\n\u003Cp\u003EPada contoh berikut, kita dapat melihat seberapa mudahnya pengulangan dapat digantikan dengan Vectorization untuk kasus penggunaan seperti ini.\u003C/p\u003E\n\u003Cp\u003EMembuat DataFrame\u003C/p\u003E\n\u003Cp\u003EDataFrame adalah data tabular dalam bentuk baris dan kolom.\u003C/p\u003E\n\u003Cp\u003EKami membuat DataFrame Pandas dengan 5 Juta baris dan 4 kolom yang diisi dengan nilai acak antara 0 dan 50.\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Eimport pandas as pd\u003Cbr\u003Edf = pd.DataFrame(np.random.randint(0, 50, size=(5000000, 4)), columns=('a', 'b', 'c', 'd'))\u003Cbr\u003Edf.shape\u003Cbr\u003E# (5000000, 5)\u003Cbr\u003Edf.head()\u003C/pre\u003E\n\u003Cp\u003EKami akan membuat kolom baru ‘ratio’ untuk menemukan rasio dari kolom ‘d’ dan ‘c’.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time \u003Cbr\u003Estart = time.time()\u003Cbr\u003E# Mengulang melalui DataFrame menggunakan iterrows\u003Cbr\u003Efor idx, row in df.iterrows():\u003Cbr\u003E # membuat kolom baru \u003Cbr\u003E df.at[idx, 'ratio'] = 100 * (row[\"d\"] / row[\"c\"]) \u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### 109 Detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Estart = time.time()\u003Cbr\u003Edf[\"ratio\"] = 100 * (df[\"d\"] / df[\"c\"])\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### 0,12 detik\u003C/pre\u003E\n\u003Cp\u003EKita dapat melihat peningkatan yang signifikan dengan DataFrame, waktu yang dibutuhkan oleh operasi Vectorization hampir 1000x lebih cepat dibandingkan dengan pengulangan di Python.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 3: Pernyataan If-else (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EKita mengimplementasikan banyak operasi yang memerlukan kita untuk menggunakan logika ‘If-else’. Kita dapat dengan mudah menggantikan logika ini dengan operasi Vectorization dalam Python.\u003C/p\u003E\n\u003Cp\u003EMari lihat contoh berikut untuk memahaminya lebih baik (kita akan menggunakan DataFrame yang telah kita buat pada kasus penggunaan 2):\u003C/p\u003E\n\u003Cp\u003EBayangkan kita ingin membuat kolom baru ‘e’ berdasarkan beberapa kondisi pada kolom yang ada ‘a’.\u003C/p\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport time \u003Cbr\u003Estart = time.time()\u003Cbr\u003E# Mengulang melalui DataFrame menggunakan iterrows\u003Cbr\u003Efor idx, row in df.iterrows():\u003Cbr\u003E if row.a == 0:\u003Cbr\u003E df.at[idx, 'e'] = row.d \u003Cbr\u003E elif (row.a &lt;= 25) &amp; (row.a &gt; 0):\u003Cbr\u003E df.at[idx, 'e'] = (row.b) - (row.c) \u003Cbr\u003E else:\u003Cbr\u003E df.at[idx, 'e'] = row.b + row.c\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E### Waktu yang dibutuhkan: 177 detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003E# menggunakan vectorization \u003Cbr\u003Estart = time.time()\u003Cbr\u003Edf['e'] = df['b'] + df['c']\u003Cbr\u003Edf.loc[df['a'] &lt;= 25, 'e'] = df['b'] - df['c']\u003Cbr\u003Edf.loc[df['a'] == 0, 'e'] = df['d']\u003Cbr\u003Eend = time.time()\u003Cbr\u003Eprint(end - start)\u003Cbr\u003E## 0,28007707595825195 detik\u003C/pre\u003E\n\u003Cp\u003EWaktu yang dibutuhkan oleh operasi Vectorization 600x lebih cepat dibandingkan dengan pengulangan Python dengan pernyataan if-else.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECase 4: Menyelesaikan Jaringan Pembelajaran Mesin/Deep Learning\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EPembelajaran Mendalam memerlukan kita untuk menyelesaikan beberapa persamaan kompleks dan itu pun untuk jutaan dan miliaran baris. Menjalankan pengulangan di Python untuk menyelesaikan persamaan-persamaan ini sangat lambat dan Vectorization adalah solusi optimal.\u003C/p\u003E\n\u003Cp\u003ESebagai contoh, untuk menghitung nilai y untuk jutaan baris dalam persamaan regresi multi-linier:\u003C/p\u003E\n\u003Cp\u003EPersamaan Regresi Linier (Gambar oleh Penulis)\u003C/p\u003E\n\u003Cp\u003EKita dapat menggantikan pengulangan dengan Vectorization.\u003C/p\u003E\n\u003Cp\u003ENilai-nilai m1, m2, m3,… ditentukan dengan menyelesaikan persamaan di atas menggunakan jutaan nilai yang sesuai dengan x1, x2, x3,… (untuk kesederhanaan, kita hanya akan melihat langkah perkalian sederhana)\u003C/p\u003E\n\u003Cp\u003EMembuat Data\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003E# menetapkan nilai awal m \u003Cbr\u003Em = np.random.rand(1,5)\u003Cbr\u003E# nilai input untuk 5 juta baris\u003Cbr\u003Ex = np.random.rand(5000000,5)\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Pengulangan\u003C/p\u003E\n\u003Cpre\u003Eimport numpy as np\u003Cbr\u003Em = np.random.rand(1, 5)\u003Cbr\u003Ex = np.random.rand(5000000, 5)\u003Cbr\u003Etotal = 0\u003Cbr\u003Etic = time.process_time()\u003Cbr\u003Efor i in range(0, 5000000):\u003Cbr\u003E total = 0\u003Cbr\u003E for j in range(0, 5):\u003Cbr\u003E total = total + x[i][j] * m[0][j] \u003Cbr\u003E\u003Cbr\u003E zer[i] = total \u003Cbr\u003Etoc = time.process_time()\u003Cbr\u003Eprint(\"Waktu komputasi = \" + str((toc - tic)) + \"detik\")\u003Cbr\u003E####Waktu komputasi = 28,228 detik\u003C/pre\u003E\n\u003Cp\u003EMenggunakan Vectorization\u003C/p\u003E\n\u003Cpre\u003Etic = time.process_time()\u003Cbr\u003E# perkalian dot\u003Cbr\u003Enp.dot(x, m.T) \u003Cbr\u003Etoc = time.process_time()\u003Cbr\u003Eprint(\"Waktu komputasi = \" + str((toc - tic)) + \"detik\")\u003Cbr\u003E####Waktu komputasi = 0,107 detik\u003C/pre\u003E\n\u003Cp\u003E`np.dot` mengimplementasikan perkalian matriks Vectorized di belakang layar. Ini 165x lebih cepat dibandingkan dengan pengulangan di Python.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ERingkasan:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDalam tiga kasus penggunaan yang dibahas, kita melihat betapa efisiennya Vectorization dibandingkan dengan pengulangan dalam Python, terutama saat bekerja dengan DataFrame Pandas dan operasi matematika kompleks.\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003E\u003Cstrong\u003EPenggunaan 1: Menemukan Jumlah Bilangan\u003C/strong\u003E\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003EDengan menggunakan pengulangan, waktu eksekusi adalah 0,14 detik.\u003C/li\u003E\n\u003Cli\u003EDengan menggunakan Vectorization (NumPy), waktu eksekusi hanya 0,008 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization membutuhkan waktu ~18x lebih sedikit dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E2. Penggunaan 2: Operasi Matematika (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan memakan waktu 177 detik untuk membuat kolom ‘ratio’ pada DataFrame.\u003C/li\u003E\n\u003Cli\u003EVectorization hanya memerlukan waktu 0,28007707595825195 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization mempercepat operasi sekitar 600x dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E3. Penggunaan 3: Pernyataan If-else (pada DataFrame)\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan membutuhkan waktu 177 detik untuk menentukan nilai kolom ‘e’ berdasarkan kondisi pada kolom ‘a’.\u003C/li\u003E\n\u003Cli\u003EVectorization hanya memerlukan waktu 0,28007707595825195 detik.\u003C/li\u003E\n\u003Cli\u003EWaktu eksekusi Vectorization sekitar 600x lebih cepat dibandingkan dengan pengulangan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E4. Penggunaan 4 (Lanjutan): Menyelesaikan Jaringan Pembelajaran Mesin/Deep Learning.\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EDalam kasus ini, Vectorization sangat optimal untuk menyelesaikan persamaan kompleks dalam jutaan dan miliaran baris.\u003C/li\u003E\n\u003Cli\u003EMemanfaatkan NumPy dan operasi vektorisasi dapat memberikan peningkatan performa yang signifikan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u003Cstrong\u003E5. Penggunaan Tambahan: Matriks Dot Product\u003C/strong\u003E\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EPengulangan memakan waktu 28,228 detik untuk melakukan perkalian dot pada dua matriks.\u003C/li\u003E\n\u003Cli\u003EVectorization menggunakan `np.dot` hanya memerlukan waktu 0,107 detik.\u003C/li\u003E\n\u003Cli\u003EVectorization mengungguli pengulangan sekitar 165x dalam hal kecepatan eksekusi.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EDalam semua kasus, penggunaan Vectorization, terutama dengan NumPy, dapat memberikan peningkatan kinerja yang luar biasa, terutama ketika bekerja dengan dataset yang besar.\u003C/p\u003E\n\u003Cp\u003Ereference : \u003Ca href=\"https://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e8b0172b9581\"\u003Ehttps://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e8b0172b9581\u003C/a\u003E\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=56adfafe5d66\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "python",
          "vectorization"
        ]
      },
      {
        "title": "Memahami Rabbit MQ dari dasar",
        "pubDate": "2023-12-27 07:36:14",
        "link": "https://nasriadzlani.medium.com/memahami-rabbit-mq-dari-dasar-06ef521c90af?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/06ef521c90af",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EPerusahaan-perusahaan kini beralih ke arah pengadopsian arsitektur berbasis mikroservis dalam aplikasi modern. Untuk menangani dan mendistribusikan beban kerja guna menjaga agar aplikasi berfungsi lancar dan cepat, penggunaan modul yang terlepas menjadi suatu kebutuhan. Message broker memungkinkan aplikasi berkomunikasi dan terlepas satu sama lain.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ adalah message broker open-source yang banyak digunakan yang membantu dalam meningkatkan skalabilitas aplikasi dengan menerapkan mekanisme antrian pesan di antara dua aplikasi. RabbitMQ menyediakan penyimpanan sementara untuk data guna mencegah kehilangan data. Antrian RabbitMQ mengambil pesan dari penerbit dan mengirimkannya ke konsumen.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ berfungsi sebagai platform perantara yang memastikan pesan terkirim ke tujuan yang tepat. Dalam artikel ini, Anda akan mempelajari tentang RabbitMQ, terminologi dasarnya, dan cara berkomunikasi melalui RabbitMQ. Anda juga akan menjelajahi langkah-langkah untuk membuat program Python yang mengirim pesan melalui Antrian RabbitMQ.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X85021YUSj1yqeEK.png\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EApa itu RabbitMQ?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ERabbitMQ adalah perangkat lunak message broker open-source yang ditulis dalam bahasa Erlang. Umumnya disebut sebagai middleware berorientasi pesan yang mengimplementasikan protokol AMQP (Advanced Message Queuing Protocol). RabbitMQ diperluas dengan arsitektur plugin untuk memberikan dukungan terhadap MQ Telemetry Transport (MQTT), Streaming Text Oriented Messaging Protocol (STOMP), dan protokol lainnya. RabbitMQ sangat skalabel dan menjamin ketersediaan data sepanjang waktu dengan arsitektur yang toleran terhadap kegagalan.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ mendukung banyak bahasa pemrograman dan dapat berjalan di berbagai lingkungan Cloud dan sistem operasi. Ini menawarkan antarmuka pengguna berbasis browser untuk pemantauan dan manajemen, dan juga dilengkapi dengan alat-alat CLI dan UI HTTP untuk operasi. RabbitMQ ringan dan mudah diterapkan di Cloud maupun on-premises. Selain itu, dapat diterapkan dalam konfigurasi terdistribusi maupun federasi untuk memberikan skalabilitas tinggi dan ketersediaan untuk memenuhi kebutuhan bisnis.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EKonsep dasar RabbitMQ:\u003C/strong\u003E\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EProducer (Penghasil): Produsen adalah pihak yang mengirimkan pesan ke sebuah antrian berdasarkan nama antrian.\u003C/li\u003E\n\u003Cli\u003EQueue (Antrian): Antrian adalah struktur data berurutan yang menjadi media pengiriman dan penyimpanan pesan.\u003C/li\u003E\n\u003Cli\u003EConsumer (Konsumen): Konsumen adalah pihak yang berlangganan dan menerima pesan dari broker, lalu menggunakan pesan tersebut untuk operasi lain yang telah ditentukan.\u003C/li\u003E\n\u003Cli\u003EExchange (Pertukaran): Pertukaran adalah titik masuk untuk broker karena mengambil pesan dari penerbit dan mengarahkan pesan-pesan tersebut ke antrian yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBroker: Ini adalah message broker yang menyediakan penyimpanan untuk data yang dihasilkan. Data dimaksudkan untuk dikonsumsi atau diterima oleh aplikasi lain yang terhubung ke broker dengan parameter atau string koneksi tertentu.\u003C/li\u003E\n\u003Cli\u003EChannel (Saluran): Saluran menawarkan koneksi ringan ke broker melalui koneksi TCP bersama.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EFitur Utama RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EBeberapa fitur utama RabbitMQ meliputi:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EPengimplementasian Terdistribusi: Pengguna dapat menerapkan RabbitMQ sebagai kluster untuk ketersediaan dan throughput yang tinggi. Kluster ini diberdayakan di beberapa zona ketersediaan dan wilayah untuk selalu tersedia.\u003C/li\u003E\n\u003Cli\u003EAlat dan Plugin: RabbitMQ menawarkan berbagai alat dan plugin untuk integrasi berkelanjutan, metrik operasional, dan integrasi dengan sistem perusahaan lainnya.\u003C/li\u003E\n\u003Cli\u003ESiap Enterprise dan Cloud: RabbitMQ ringan dan mudah diterapkan di cloud publik maupun pribadi dengan menggunakan otorisasi otentikasi yang dapat dipasang.\u003C/li\u003E\n\u003Cli\u003EPesan Asinkron: RabbitMQ mendukung berbagai protokol komunikasi, antrian pesan, pengakuan pengiriman, perutean ke antrian, dan berbagai jenis pertukaran.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EApa itu Antrian RabbitMQ?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ adalah struktur data berurutan di mana item dapat dimasukkan (enqueued) di bagian belakang atau diambil (dequeued) dari bagian depan. Penerbit dan Konsumen berkomunikasi menggunakan mekanisme penyimpanan mirip antrian. Antrian RabbitMQ mengikuti prinsip FIFO (First In First Out), tetapi juga menawarkan fitur-fitur antrian lainnya seperti prioritas dan re-queueing untuk mengubah urutan.\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ memiliki nama sehingga aplikasi dapat merujuk padanya. Aplikasi memilih Antrian RabbitMQ berdasarkan nama atau meminta broker untuk membuat nama untuk membedakan dua Antrian RabbitMQ satu sama lain.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EProperti Antrian RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ memiliki properti yang menentukan perilaku antrian tersebut. Ada beberapa properti yang wajib dan opsional dari Antrian RabbitMQ, seperti yang tercantum di bawah ini:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EAntrian RabbitMQ harus memiliki nama.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ bersifat tahan lama agar dapat bertahan saat restart broker.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ dapat bersifat Eksklusif, digunakan oleh satu koneksi saja, dan Antrian akan dihapus ketika koneksi ditutup.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ akan otomatis dihapus ketika konsumen terakhir berhenti berlangganan.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ memiliki argumen opsional seperti batas panjang antrian, TTL pesan, dll.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EBagaimana Antrian Pesan RabbitMQ Bekerja?\u003C/strong\u003E\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*83iwuOnc2V1mjQHW.png\"\u003E\u003Cfigcaption\u003Esrc:\u003Ca href=\"https://medium.com/turkcell/message-queues-in-action-rabbitmq-explained-98a002b73750\"\u003Ehttps://medium.com/turkcell/message-queues-in-action-rabbitmq-explained-98a002b73750\u003C/a\u003E\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003EDalam bagian ini, Anda akan mempelajari bagaimana alur pesan RabbitMQ terjadi. Berikut adalah prosedur berikut:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EProdusen pertama kali menerbitkan pesan ke pertukaran dengan tipe tertentu.\u003C/li\u003E\n\u003Cli\u003EBegitu pertukaran menerima pesan, pertukaran bertanggung jawab untuk merute pesan. Pertukaran merute pesan dengan mempertimbangkan parameter lain seperti tipe pertukaran, kunci rute, dll.\u003C/li\u003E\n\u003Cli\u003ESekarang, pengikatan (binding) dibuat dari pertukaran ke Antrian RabbitMQ. Setiap Antrian memiliki nama yang membantu membedakannya. Kemudian pertukaran merute pesan ke Antrian berdasarkan atribut pesan.\u003C/li\u003E\n\u003Cli\u003EBegitu pesan dimasukkan dalam Antrian, pesan tetap berada di sana sampai ditangani oleh konsumen.\u003C/li\u003E\n\u003Cli\u003EKonsumen kemudian menangani pesan dari Antrian RabbitMQ.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003ELangkah-langkah Mengirim Pesan Melalui RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ESekarang bahwa Anda telah memahami apa itu RabbitMQ, terminologi dasarnya, dan bagaimana pesan mengalir melalui Antrian RabbitMQ. Dalam bagian ini, Anda akan melalui proses mengirim pesan melalui RabbitMQ menggunakan Python. Berikut adalah langkah-langkah untuk mengirim pesan melalui RabbitMQ:\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003EMengirim Pesan:\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003EGunakan Pika, klien Python untuk RabbitMQ. Pastikan untuk menginstalnya dengan menjalankan `pip install pika`.\u003C/li\u003E\n\u003Cli\u003EImport modul Pika dalam program Python Anda.\u003C/li\u003E\n\u003Cli\u003EAtur koneksi ke server RabbitMQ menggunakan host, port, dan kredensial yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBuka saluran (channel) untuk komunikasi dengan RabbitMQ.\u003C/li\u003E\n\u003Cli\u003EDeklarasikan antrian RabbitMQ yang akan digunakan.\u003C/li\u003E\n\u003Cli\u003EKirim pesan ke antrian menggunakan saluran yang telah dibuka.\u003C/li\u003E\n\u003Cli\u003ETutup saluran dan koneksi setelah mengirim pesan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E2. Menerima Pesan:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EBuat program terpisah untuk menerima pesan.\u003C/li\u003E\n\u003Cli\u003EImport modul Pika.\u003C/li\u003E\n\u003Cli\u003EAtur koneksi ke server RabbitMQ dengan parameter yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBuka saluran untuk berkomunikasi.\u003C/li\u003E\n\u003Cli\u003EDeklarasikan antrian RabbitMQ yang sesuai dengan antrian pengirim.\u003C/li\u003E\n\u003Cli\u003ETentukan fungsi callback yang akan menangani pesan saat diterima.\u003C/li\u003E\n\u003Cli\u003EDaftarkan fungsi callback sebagai konsumen pada antrian.\u003C/li\u003E\n\u003Cli\u003EMulai konsumsi pesan dari antrian.\u003C/li\u003E\n\u003Cli\u003ETunggu atau terus jalankan program agar tetap mendengarkan pesan yang masuk.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EPastikan untuk menyesuaikan kode Python sesuai dengan kebutuhan aplikasi Anda. Dengan langkah-langkah ini, Anda akan dapat mengirim dan menerima pesan melalui RabbitMQ menggunakan bahasa pemrograman Python.\u003C/p\u003E\n\u003Cp\u003Euntuk penjelasan lebih lanjut bisa ke \u003Ca href=\"https://medium.com/@nasriadzlani/rabbitmq-on-docker-and-python-300e449fcc8c\"\u003Ehttps://medium.com/@nasriadzlani/rabbitmq-on-docker-and-python-300e449fcc8c\u003C/a\u003E\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EKesimpulan\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDengan memahami konsep dasar RabbitMQ, Anda sekarang memiliki dasar yang kokoh untuk memahami bagaimana pesan dikirim dan diterima melalui Antrian RabbitMQ. Penerapan teknologi ini, terutama dengan menggunakan bahasa pemrograman Python dan klien Pika, memberikan Anda kemampuan untuk mengoptimalkan proses komunikasi dalam aplikasi Anda.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ sebagai message broker membantu mengatasi tantangan dalam pengembangan aplikasi yang bersifat terdistribusi, memberikan skalabilitas, dan memastikan ketersediaan data. Dengan memanfaatkan konsep produsen, antrian, dan konsumen, Anda dapat merancang sistem yang efisien dan dapat diandalkan.\u003C/p\u003E\n\u003Cp\u003ETeruslah eksplorasi dan penerapan RabbitMQ dalam proyek-proyek Anda untuk mengoptimalkan arsitektur mikroservis, mengimplementasikan pesan asinkron, dan meningkatkan kinerja aplikasi secara keseluruhan. Dengan pengetahuan ini, semoga Anda dapat menghadapi proyek-proyek teknologi dengan lebih percaya diri dan efektif. Selamat mengembangkan!\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=06ef521c90af\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EPerusahaan-perusahaan kini beralih ke arah pengadopsian arsitektur berbasis mikroservis dalam aplikasi modern. Untuk menangani dan mendistribusikan beban kerja guna menjaga agar aplikasi berfungsi lancar dan cepat, penggunaan modul yang terlepas menjadi suatu kebutuhan. Message broker memungkinkan aplikasi berkomunikasi dan terlepas satu sama lain.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ adalah message broker open-source yang banyak digunakan yang membantu dalam meningkatkan skalabilitas aplikasi dengan menerapkan mekanisme antrian pesan di antara dua aplikasi. RabbitMQ menyediakan penyimpanan sementara untuk data guna mencegah kehilangan data. Antrian RabbitMQ mengambil pesan dari penerbit dan mengirimkannya ke konsumen.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ berfungsi sebagai platform perantara yang memastikan pesan terkirim ke tujuan yang tepat. Dalam artikel ini, Anda akan mempelajari tentang RabbitMQ, terminologi dasarnya, dan cara berkomunikasi melalui RabbitMQ. Anda juga akan menjelajahi langkah-langkah untuk membuat program Python yang mengirim pesan melalui Antrian RabbitMQ.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X85021YUSj1yqeEK.png\"\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003EApa itu RabbitMQ?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ERabbitMQ adalah perangkat lunak message broker open-source yang ditulis dalam bahasa Erlang. Umumnya disebut sebagai middleware berorientasi pesan yang mengimplementasikan protokol AMQP (Advanced Message Queuing Protocol). RabbitMQ diperluas dengan arsitektur plugin untuk memberikan dukungan terhadap MQ Telemetry Transport (MQTT), Streaming Text Oriented Messaging Protocol (STOMP), dan protokol lainnya. RabbitMQ sangat skalabel dan menjamin ketersediaan data sepanjang waktu dengan arsitektur yang toleran terhadap kegagalan.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ mendukung banyak bahasa pemrograman dan dapat berjalan di berbagai lingkungan Cloud dan sistem operasi. Ini menawarkan antarmuka pengguna berbasis browser untuk pemantauan dan manajemen, dan juga dilengkapi dengan alat-alat CLI dan UI HTTP untuk operasi. RabbitMQ ringan dan mudah diterapkan di Cloud maupun on-premises. Selain itu, dapat diterapkan dalam konfigurasi terdistribusi maupun federasi untuk memberikan skalabilitas tinggi dan ketersediaan untuk memenuhi kebutuhan bisnis.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EKonsep dasar RabbitMQ:\u003C/strong\u003E\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EProducer (Penghasil): Produsen adalah pihak yang mengirimkan pesan ke sebuah antrian berdasarkan nama antrian.\u003C/li\u003E\n\u003Cli\u003EQueue (Antrian): Antrian adalah struktur data berurutan yang menjadi media pengiriman dan penyimpanan pesan.\u003C/li\u003E\n\u003Cli\u003EConsumer (Konsumen): Konsumen adalah pihak yang berlangganan dan menerima pesan dari broker, lalu menggunakan pesan tersebut untuk operasi lain yang telah ditentukan.\u003C/li\u003E\n\u003Cli\u003EExchange (Pertukaran): Pertukaran adalah titik masuk untuk broker karena mengambil pesan dari penerbit dan mengarahkan pesan-pesan tersebut ke antrian yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBroker: Ini adalah message broker yang menyediakan penyimpanan untuk data yang dihasilkan. Data dimaksudkan untuk dikonsumsi atau diterima oleh aplikasi lain yang terhubung ke broker dengan parameter atau string koneksi tertentu.\u003C/li\u003E\n\u003Cli\u003EChannel (Saluran): Saluran menawarkan koneksi ringan ke broker melalui koneksi TCP bersama.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EFitur Utama RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EBeberapa fitur utama RabbitMQ meliputi:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EPengimplementasian Terdistribusi: Pengguna dapat menerapkan RabbitMQ sebagai kluster untuk ketersediaan dan throughput yang tinggi. Kluster ini diberdayakan di beberapa zona ketersediaan dan wilayah untuk selalu tersedia.\u003C/li\u003E\n\u003Cli\u003EAlat dan Plugin: RabbitMQ menawarkan berbagai alat dan plugin untuk integrasi berkelanjutan, metrik operasional, dan integrasi dengan sistem perusahaan lainnya.\u003C/li\u003E\n\u003Cli\u003ESiap Enterprise dan Cloud: RabbitMQ ringan dan mudah diterapkan di cloud publik maupun pribadi dengan menggunakan otorisasi otentikasi yang dapat dipasang.\u003C/li\u003E\n\u003Cli\u003EPesan Asinkron: RabbitMQ mendukung berbagai protokol komunikasi, antrian pesan, pengakuan pengiriman, perutean ke antrian, dan berbagai jenis pertukaran.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EApa itu Antrian RabbitMQ?\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ adalah struktur data berurutan di mana item dapat dimasukkan (enqueued) di bagian belakang atau diambil (dequeued) dari bagian depan. Penerbit dan Konsumen berkomunikasi menggunakan mekanisme penyimpanan mirip antrian. Antrian RabbitMQ mengikuti prinsip FIFO (First In First Out), tetapi juga menawarkan fitur-fitur antrian lainnya seperti prioritas dan re-queueing untuk mengubah urutan.\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ memiliki nama sehingga aplikasi dapat merujuk padanya. Aplikasi memilih Antrian RabbitMQ berdasarkan nama atau meminta broker untuk membuat nama untuk membedakan dua Antrian RabbitMQ satu sama lain.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EProperti Antrian RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EAntrian RabbitMQ memiliki properti yang menentukan perilaku antrian tersebut. Ada beberapa properti yang wajib dan opsional dari Antrian RabbitMQ, seperti yang tercantum di bawah ini:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EAntrian RabbitMQ harus memiliki nama.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ bersifat tahan lama agar dapat bertahan saat restart broker.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ dapat bersifat Eksklusif, digunakan oleh satu koneksi saja, dan Antrian akan dihapus ketika koneksi ditutup.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ akan otomatis dihapus ketika konsumen terakhir berhenti berlangganan.\u003C/li\u003E\n\u003Cli\u003EAntrian RabbitMQ memiliki argumen opsional seperti batas panjang antrian, TTL pesan, dll.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003EBagaimana Antrian Pesan RabbitMQ Bekerja?\u003C/strong\u003E\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*83iwuOnc2V1mjQHW.png\"\u003E\u003Cfigcaption\u003Esrc:\u003Ca href=\"https://medium.com/turkcell/message-queues-in-action-rabbitmq-explained-98a002b73750\"\u003Ehttps://medium.com/turkcell/message-queues-in-action-rabbitmq-explained-98a002b73750\u003C/a\u003E\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003EDalam bagian ini, Anda akan mempelajari bagaimana alur pesan RabbitMQ terjadi. Berikut adalah prosedur berikut:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003EProdusen pertama kali menerbitkan pesan ke pertukaran dengan tipe tertentu.\u003C/li\u003E\n\u003Cli\u003EBegitu pertukaran menerima pesan, pertukaran bertanggung jawab untuk merute pesan. Pertukaran merute pesan dengan mempertimbangkan parameter lain seperti tipe pertukaran, kunci rute, dll.\u003C/li\u003E\n\u003Cli\u003ESekarang, pengikatan (binding) dibuat dari pertukaran ke Antrian RabbitMQ. Setiap Antrian memiliki nama yang membantu membedakannya. Kemudian pertukaran merute pesan ke Antrian berdasarkan atribut pesan.\u003C/li\u003E\n\u003Cli\u003EBegitu pesan dimasukkan dalam Antrian, pesan tetap berada di sana sampai ditangani oleh konsumen.\u003C/li\u003E\n\u003Cli\u003EKonsumen kemudian menangani pesan dari Antrian RabbitMQ.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003E\u003Cstrong\u003ELangkah-langkah Mengirim Pesan Melalui RabbitMQ\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003ESekarang bahwa Anda telah memahami apa itu RabbitMQ, terminologi dasarnya, dan bagaimana pesan mengalir melalui Antrian RabbitMQ. Dalam bagian ini, Anda akan melalui proses mengirim pesan melalui RabbitMQ menggunakan Python. Berikut adalah langkah-langkah untuk mengirim pesan melalui RabbitMQ:\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003EMengirim Pesan:\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003EGunakan Pika, klien Python untuk RabbitMQ. Pastikan untuk menginstalnya dengan menjalankan `pip install pika`.\u003C/li\u003E\n\u003Cli\u003EImport modul Pika dalam program Python Anda.\u003C/li\u003E\n\u003Cli\u003EAtur koneksi ke server RabbitMQ menggunakan host, port, dan kredensial yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBuka saluran (channel) untuk komunikasi dengan RabbitMQ.\u003C/li\u003E\n\u003Cli\u003EDeklarasikan antrian RabbitMQ yang akan digunakan.\u003C/li\u003E\n\u003Cli\u003EKirim pesan ke antrian menggunakan saluran yang telah dibuka.\u003C/li\u003E\n\u003Cli\u003ETutup saluran dan koneksi setelah mengirim pesan.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E2. Menerima Pesan:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EBuat program terpisah untuk menerima pesan.\u003C/li\u003E\n\u003Cli\u003EImport modul Pika.\u003C/li\u003E\n\u003Cli\u003EAtur koneksi ke server RabbitMQ dengan parameter yang sesuai.\u003C/li\u003E\n\u003Cli\u003EBuka saluran untuk berkomunikasi.\u003C/li\u003E\n\u003Cli\u003EDeklarasikan antrian RabbitMQ yang sesuai dengan antrian pengirim.\u003C/li\u003E\n\u003Cli\u003ETentukan fungsi callback yang akan menangani pesan saat diterima.\u003C/li\u003E\n\u003Cli\u003EDaftarkan fungsi callback sebagai konsumen pada antrian.\u003C/li\u003E\n\u003Cli\u003EMulai konsumsi pesan dari antrian.\u003C/li\u003E\n\u003Cli\u003ETunggu atau terus jalankan program agar tetap mendengarkan pesan yang masuk.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EPastikan untuk menyesuaikan kode Python sesuai dengan kebutuhan aplikasi Anda. Dengan langkah-langkah ini, Anda akan dapat mengirim dan menerima pesan melalui RabbitMQ menggunakan bahasa pemrograman Python.\u003C/p\u003E\n\u003Cp\u003Euntuk penjelasan lebih lanjut bisa ke \u003Ca href=\"https://medium.com/@nasriadzlani/rabbitmq-on-docker-and-python-300e449fcc8c\"\u003Ehttps://medium.com/@nasriadzlani/rabbitmq-on-docker-and-python-300e449fcc8c\u003C/a\u003E\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EKesimpulan\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EDengan memahami konsep dasar RabbitMQ, Anda sekarang memiliki dasar yang kokoh untuk memahami bagaimana pesan dikirim dan diterima melalui Antrian RabbitMQ. Penerapan teknologi ini, terutama dengan menggunakan bahasa pemrograman Python dan klien Pika, memberikan Anda kemampuan untuk mengoptimalkan proses komunikasi dalam aplikasi Anda.\u003C/p\u003E\n\u003Cp\u003ERabbitMQ sebagai message broker membantu mengatasi tantangan dalam pengembangan aplikasi yang bersifat terdistribusi, memberikan skalabilitas, dan memastikan ketersediaan data. Dengan memanfaatkan konsep produsen, antrian, dan konsumen, Anda dapat merancang sistem yang efisien dan dapat diandalkan.\u003C/p\u003E\n\u003Cp\u003ETeruslah eksplorasi dan penerapan RabbitMQ dalam proyek-proyek Anda untuk mengoptimalkan arsitektur mikroservis, mengimplementasikan pesan asinkron, dan meningkatkan kinerja aplikasi secara keseluruhan. Dengan pengetahuan ini, semoga Anda dapat menghadapi proyek-proyek teknologi dengan lebih percaya diri dan efektif. Selamat mengembangkan!\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=06ef521c90af\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "rabbitmq",
          "message-queue",
          "message-broker",
          "rabbits"
        ]
      },
      {
        "title": "How To Create Library and publish on PyPI python",
        "pubDate": "2023-11-10 04:12:44",
        "link": "https://nasriadzlani.medium.com/how-to-create-library-and-publish-on-pypi-python-33a8c1d3bf0b?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/33a8c1d3bf0b",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EIn the world of Python programming, creating reusable and well-organized code is crucial for efficient development. Python libraries play a pivotal role in achieving this goal, as they allow developers to package and distribute their code in a way that can be easily shared and utilized by others. In this article, we’ll explore how to build a Python library using Poetry, a user-friendly dependency management and packaging tool. Whether you’re new to programming or just starting with Python, this guide will help you get started on the path to creating your own Python library.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/0*9Uz9gcGR1Y0_zNJC.png\"\u003E\u003Cfigcaption\u003Epoetry logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch4\u003EWhat is a Python Library?\u003C/h4\u003E\n\u003Cp\u003EA Python library is a collection of functions, classes, and modules that are written to perform specific tasks. Libraries are designed to be reusable, meaning they can be easily imported into other projects, saving developers time and effort by eliminating the need to reinvent the wheel for common tasks.\u003C/p\u003E\n\u003Ch4\u003EGetting Started with Poetry\u003C/h4\u003E\n\u003Cp\u003EPoetry is a modern Python dependency management and packaging tool that simplifies the process of creating and managing Python projects, including libraries.\u003C/p\u003E\n\u003Cp\u003EInstall on Linux, MacOS, WSL (Windows)\u003C/p\u003E\n\u003Cpre\u003Ecurl -sSL https://install.python-poetry.org | python3 -\u003C/pre\u003E\n\u003Cp\u003EInstall on PowerShell (Windows)\u003C/p\u003E\n\u003Cpre\u003E(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\u003C/pre\u003E\n\u003Cp\u003Eafter installing, you can check poetry version\u003C/p\u003E\n\u003Cpre\u003Epoetry --version\u003C/pre\u003E\n\u003Ch4\u003ESetting Up Your Library Project\u003C/h4\u003E\n\u003Cp\u003EOpen your terminal and navigate to the directory where you want to create your library. Use the following command to initiate a new Poetry project:\u003C/p\u003E\n\u003Cpre\u003Epoetry new custom-log\u003C/pre\u003E\n\u003Cp\u003Eafter execute above command, poetry will create folder with structure like this:\u003C/p\u003E\n\u003Cpre\u003E.\u003Cbr\u003E├── README.md\u003Cbr\u003E├── custom_log\u003Cbr\u003E│   └── __init__.py\u003Cbr\u003E├── pyproject.toml\u003Cbr\u003E└── tests\u003Cbr\u003E    └── __init__.py\u003C/pre\u003E\n\u003Cp\u003Eopen your project directory and create your poetry virtual environment,\u003C/p\u003E\n\u003Cpre\u003Ecd custom-log\u003Cbr\u003Epoetry shell\u003C/pre\u003E\n\u003Cpre\u003Enasri@aliendev:~/Research/custom-log$ poetry shell\u003Cbr\u003ECreating virtualenv custom-log-vdAXMe7h-py3.10 in /home/nasri/.cache/pypoetry/virtualenvs\u003Cbr\u003ESpawning shell within /home/nasri/.cache/pypoetry/virtualenvs/custom-log-vdAXMe7h-py3.10\u003C/pre\u003E\n\u003Ch4\u003EDefining Dependencies\u003C/h4\u003E\n\u003Cp\u003ENavigate into your newly created project directory using cd custom-log and open the pyproject.toml file. This is where you'll define your project's dependencies. Add your required dependencies:\u003C/p\u003E\n\u003Cpre\u003E\u003Cbr\u003E[tool.poetry.dependencies]\u003Cbr\u003Epython = \"^3.10\"\u003Cbr\u003Eloguru = \"^0.7.0\"\u003C/pre\u003E\n\u003Cpre\u003Epoetry install\u003C/pre\u003E\n\u003Cp\u003Eor use command:\u003C/p\u003E\n\u003Cpre\u003Epoetry add loguru\u003C/pre\u003E\n\u003Ch4\u003EStructuring Your Library\u003C/h4\u003E\n\u003Cp\u003ECreate a directory structure for your library. You can place your library code inside a subdirectory, such as custom-log, and add an empty __init__.py file to make it a package.\u003C/p\u003E\n\u003Ch4\u003EWriting Your Library Code\u003C/h4\u003E\n\u003Cp\u003Ecreate main.py file and add example code like this:\u003C/p\u003E\n\u003Cpre\u003E# main.py file\u003Cbr\u003Eimport functools\u003Cbr\u003Efrom loguru import logger\u003Cbr\u003E\u003Cbr\u003Edef custom_log(func):\u003Cbr\u003E    @functools.wraps(func)\u003Cbr\u003E    def wrapper(*args, **kwargs):\u003Cbr\u003E        logger.info(f\"Calling {func.__name__} with arguments {args} and kwargs {kwargs}\")\u003Cbr\u003E        result = func(*args, **kwargs)\u003Cbr\u003E        logger.info(f\"{func.__name__} returned {result}\")\u003Cbr\u003E        return result\u003Cbr\u003E    return wrapper\u003C/pre\u003E\n\u003Cp\u003Ecreate simplify user to access function, open your __init__.py and add this syntax\u003C/p\u003E\n\u003Cpre\u003Efrom custom_log.main import custom_log\u003C/pre\u003E\n\u003Ch4\u003EPackaging Your Library\u003C/h4\u003E\n\u003Cp\u003EPoetry simplifies packaging by automatically generating a pyproject.toml file for your project. To create a distributable package, run the following command:\u003C/p\u003E\n\u003Cpre\u003Epoetry build\u003C/pre\u003E\n\u003Cp\u003EThis will generate a dist directory containing your packaged library.\u003C/p\u003E\n\u003Ch4\u003EPublishing Your Library\u003C/h4\u003E\n\u003Cp\u003EBefore publishing your library, make sure you have an account on the Python Package Index (PyPI). You can create one at \u003Ca href=\"https://pypi.org/account/register/\"\u003Ehttps://pypi.org/account/register/\u003C/a\u003E. Once you have an account,\u003C/p\u003E\n\u003Cp\u003Ecreate API Token on \u003Ca href=\"https://pypi.org/manage/account/token/\"\u003Ehttps://pypi.org/manage/account/token/\u003C/a\u003E and select all project for scope\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-9YO66dd5Vc_Nc-LqSUllg.png\"\u003E\u003C/figure\u003E\u003Cp\u003Eadd token on yout device use this command:\u003C/p\u003E\n\u003Cpre\u003Epoetry config pypi-token.pypi &lt;your token&gt;\u003C/pre\u003E\n\u003Cp\u003Eyou can publish your library using the following command:\u003C/p\u003E\n\u003Cpre\u003Epoetry publish --build\u003C/pre\u003E\n\u003Ch4\u003EConclusion\u003C/h4\u003E\n\u003Cp\u003EPoetry, as introduced in this guide, emerged as a user-friendly ally, streamlining the complexities of Python project development. The article’s step-by-step approach, accompanied by clear examples, provided a solid foundation for creating efficient and shareable Python libraries.\u003C/p\u003E\n\u003Cp\u003EBy delving into the intricacies of setting up a library project, defining dependencies, and structuring code, the guide empowered readers to embark on their own library-building journeys. The practical demonstration of creating a custom logging function using the loguru library illustrated real-world application, enhancing the learning experience.\u003C/p\u003E\n\u003Cp\u003ECrucially, the article didn’t stop at code creation; it extended its guidance to packaging and publishing on the Python Package Index (PyPI). This final step underscores the article’s commitment to not only fostering coding proficiency but also encouraging the sharing of valuable resources with the wider Python community.\u003C/p\u003E\n\u003Cp\u003EIn essence, this guide equips readers with the knowledge and tools needed to contribute to the collaborative spirit of Python programming. Whether one is just starting or seeking to enhance their development practices, the journey from initiation to publication outlined here ensures that building and sharing Python libraries becomes an accessible and rewarding endeavor.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=33a8c1d3bf0b\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EIn the world of Python programming, creating reusable and well-organized code is crucial for efficient development. Python libraries play a pivotal role in achieving this goal, as they allow developers to package and distribute their code in a way that can be easily shared and utilized by others. In this article, we’ll explore how to build a Python library using Poetry, a user-friendly dependency management and packaging tool. Whether you’re new to programming or just starting with Python, this guide will help you get started on the path to creating your own Python library.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/0*9Uz9gcGR1Y0_zNJC.png\"\u003E\u003Cfigcaption\u003Epoetry logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch4\u003EWhat is a Python Library?\u003C/h4\u003E\n\u003Cp\u003EA Python library is a collection of functions, classes, and modules that are written to perform specific tasks. Libraries are designed to be reusable, meaning they can be easily imported into other projects, saving developers time and effort by eliminating the need to reinvent the wheel for common tasks.\u003C/p\u003E\n\u003Ch4\u003EGetting Started with Poetry\u003C/h4\u003E\n\u003Cp\u003EPoetry is a modern Python dependency management and packaging tool that simplifies the process of creating and managing Python projects, including libraries.\u003C/p\u003E\n\u003Cp\u003EInstall on Linux, MacOS, WSL (Windows)\u003C/p\u003E\n\u003Cpre\u003Ecurl -sSL https://install.python-poetry.org | python3 -\u003C/pre\u003E\n\u003Cp\u003EInstall on PowerShell (Windows)\u003C/p\u003E\n\u003Cpre\u003E(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\u003C/pre\u003E\n\u003Cp\u003Eafter installing, you can check poetry version\u003C/p\u003E\n\u003Cpre\u003Epoetry --version\u003C/pre\u003E\n\u003Ch4\u003ESetting Up Your Library Project\u003C/h4\u003E\n\u003Cp\u003EOpen your terminal and navigate to the directory where you want to create your library. Use the following command to initiate a new Poetry project:\u003C/p\u003E\n\u003Cpre\u003Epoetry new custom-log\u003C/pre\u003E\n\u003Cp\u003Eafter execute above command, poetry will create folder with structure like this:\u003C/p\u003E\n\u003Cpre\u003E.\u003Cbr\u003E├── README.md\u003Cbr\u003E├── custom_log\u003Cbr\u003E│   └── __init__.py\u003Cbr\u003E├── pyproject.toml\u003Cbr\u003E└── tests\u003Cbr\u003E    └── __init__.py\u003C/pre\u003E\n\u003Cp\u003Eopen your project directory and create your poetry virtual environment,\u003C/p\u003E\n\u003Cpre\u003Ecd custom-log\u003Cbr\u003Epoetry shell\u003C/pre\u003E\n\u003Cpre\u003Enasri@aliendev:~/Research/custom-log$ poetry shell\u003Cbr\u003ECreating virtualenv custom-log-vdAXMe7h-py3.10 in /home/nasri/.cache/pypoetry/virtualenvs\u003Cbr\u003ESpawning shell within /home/nasri/.cache/pypoetry/virtualenvs/custom-log-vdAXMe7h-py3.10\u003C/pre\u003E\n\u003Ch4\u003EDefining Dependencies\u003C/h4\u003E\n\u003Cp\u003ENavigate into your newly created project directory using cd custom-log and open the pyproject.toml file. This is where you'll define your project's dependencies. Add your required dependencies:\u003C/p\u003E\n\u003Cpre\u003E\u003Cbr\u003E[tool.poetry.dependencies]\u003Cbr\u003Epython = \"^3.10\"\u003Cbr\u003Eloguru = \"^0.7.0\"\u003C/pre\u003E\n\u003Cpre\u003Epoetry install\u003C/pre\u003E\n\u003Cp\u003Eor use command:\u003C/p\u003E\n\u003Cpre\u003Epoetry add loguru\u003C/pre\u003E\n\u003Ch4\u003EStructuring Your Library\u003C/h4\u003E\n\u003Cp\u003ECreate a directory structure for your library. You can place your library code inside a subdirectory, such as custom-log, and add an empty __init__.py file to make it a package.\u003C/p\u003E\n\u003Ch4\u003EWriting Your Library Code\u003C/h4\u003E\n\u003Cp\u003Ecreate main.py file and add example code like this:\u003C/p\u003E\n\u003Cpre\u003E# main.py file\u003Cbr\u003Eimport functools\u003Cbr\u003Efrom loguru import logger\u003Cbr\u003E\u003Cbr\u003Edef custom_log(func):\u003Cbr\u003E    @functools.wraps(func)\u003Cbr\u003E    def wrapper(*args, **kwargs):\u003Cbr\u003E        logger.info(f\"Calling {func.__name__} with arguments {args} and kwargs {kwargs}\")\u003Cbr\u003E        result = func(*args, **kwargs)\u003Cbr\u003E        logger.info(f\"{func.__name__} returned {result}\")\u003Cbr\u003E        return result\u003Cbr\u003E    return wrapper\u003C/pre\u003E\n\u003Cp\u003Ecreate simplify user to access function, open your __init__.py and add this syntax\u003C/p\u003E\n\u003Cpre\u003Efrom custom_log.main import custom_log\u003C/pre\u003E\n\u003Ch4\u003EPackaging Your Library\u003C/h4\u003E\n\u003Cp\u003EPoetry simplifies packaging by automatically generating a pyproject.toml file for your project. To create a distributable package, run the following command:\u003C/p\u003E\n\u003Cpre\u003Epoetry build\u003C/pre\u003E\n\u003Cp\u003EThis will generate a dist directory containing your packaged library.\u003C/p\u003E\n\u003Ch4\u003EPublishing Your Library\u003C/h4\u003E\n\u003Cp\u003EBefore publishing your library, make sure you have an account on the Python Package Index (PyPI). You can create one at \u003Ca href=\"https://pypi.org/account/register/\"\u003Ehttps://pypi.org/account/register/\u003C/a\u003E. Once you have an account,\u003C/p\u003E\n\u003Cp\u003Ecreate API Token on \u003Ca href=\"https://pypi.org/manage/account/token/\"\u003Ehttps://pypi.org/manage/account/token/\u003C/a\u003E and select all project for scope\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-9YO66dd5Vc_Nc-LqSUllg.png\"\u003E\u003C/figure\u003E\u003Cp\u003Eadd token on yout device use this command:\u003C/p\u003E\n\u003Cpre\u003Epoetry config pypi-token.pypi &lt;your token&gt;\u003C/pre\u003E\n\u003Cp\u003Eyou can publish your library using the following command:\u003C/p\u003E\n\u003Cpre\u003Epoetry publish --build\u003C/pre\u003E\n\u003Ch4\u003EConclusion\u003C/h4\u003E\n\u003Cp\u003EPoetry, as introduced in this guide, emerged as a user-friendly ally, streamlining the complexities of Python project development. The article’s step-by-step approach, accompanied by clear examples, provided a solid foundation for creating efficient and shareable Python libraries.\u003C/p\u003E\n\u003Cp\u003EBy delving into the intricacies of setting up a library project, defining dependencies, and structuring code, the guide empowered readers to embark on their own library-building journeys. The practical demonstration of creating a custom logging function using the loguru library illustrated real-world application, enhancing the learning experience.\u003C/p\u003E\n\u003Cp\u003ECrucially, the article didn’t stop at code creation; it extended its guidance to packaging and publishing on the Python Package Index (PyPI). This final step underscores the article’s commitment to not only fostering coding proficiency but also encouraging the sharing of valuable resources with the wider Python community.\u003C/p\u003E\n\u003Cp\u003EIn essence, this guide equips readers with the knowledge and tools needed to contribute to the collaborative spirit of Python programming. Whether one is just starting or seeking to enhance their development practices, the journey from initiation to publication outlined here ensures that building and sharing Python libraries becomes an accessible and rewarding endeavor.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=33a8c1d3bf0b\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "poerty",
          "software-developer",
          "python-libraries",
          "python"
        ]
      },
      {
        "title": "How to Access a Local Server from Public Using a Private VPN (English Version)",
        "pubDate": "2023-08-08 17:59:36",
        "link": "https://nasriadzlani.medium.com/how-to-access-a-local-server-from-public-using-a-private-vpn-english-version-8e5013a9c750?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/8e5013a9c750",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EHey folks, some time ago, I encountered an issue when trying to access my private server from a distant location. Many of us might immediately think of using ngrok, a popular service that connects our local server to the public by accessing the domain provided by ngrok (though it’s customizable, it’s not free). However, have you ever felt that there might be another more robust and suitable option for this need?\u003C/p\u003E\n\u003Cp\u003EOut of curiosity, I began looking for ngrok alternatives that might not be widely known. During this search, I discovered an enlightening solution named Tailscale.\u003C/p\u003E\n\u003Cp\u003EIn this article, I’d like to share the story of this fascinating discovery. Why Tailscale could be an impressive choice, how it operates so well, and of course, how this can be a revelation for digital buddies like you. So, are you ready to dive into this story with me? Let’s kick off our digital adventure this time, shall we?\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*cQBpxQwqa0cm9rHF.png\"\u003E\u003C/figure\u003E\u003Cp\u003ETailscale is a platform that allows you to easily create a virtual private network (VPN) based on WireGuard technology. Unlike traditional VPNs which require complex configurations, Tailscale offers a simpler approach, making it easier to set up and use.\u003C/p\u003E\n\u003Cp\u003ESome advantages of Tailscale include:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003ETailscale utilizes WireGuard, a modern VPN protocol designed for high performance and strong security.\u003C/li\u003E\n\u003Cli\u003EIt employs authentication based on Google, Microsoft, or other accounts for setup automation and access management.\u003C/li\u003E\n\u003Cli\u003EThere’s no need for a central server or complicated NAT traversal. Each node communicates directly with another node when possible.\u003C/li\u003E\n\u003Cli\u003EAll traffic is encrypted, and each node’s authentication is securely conducted.\u003C/li\u003E\n\u003Cli\u003EIt’s available for various operating systems including Linux, Windows, macOS, iOS, and Android.”\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003ETailscale Installation Steps (for Ubuntu server):\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EFirst, download Tailscale here: \u003Ca href=\"https://tailscale.com/download/\"\u003Ehttps://tailscale.com/download/\u003C/a\u003E\n\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*NvWnoSMP6ntwUQ2Z.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\n\u003Cli\u003EYou can install according to the device you are using.\u003C/li\u003E\n\u003Cli\u003EDon’t forget to log in / register for a Tailscale account.\u003C/li\u003E\n\u003Cli\u003EIt’s recommended to use SSO (Single Sign-On).\u003C/li\u003E\n\u003Cli\u003ERun Tailscale and then log in through the browser using the command below:\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cblockquote\u003Esudo tailscale up\u003C/blockquote\u003E\n\u003Cul\u003E\u003Cli\u003EIf ‘Success’ appears, then it has been successful. Check on the dashboard at \u003Ca href=\"https://login.tailscale.com/admin/machines\"\u003Ehttps://login.tailscale.com/admin/machines\u003C/a\u003E It should appear somewhat like this:\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*ztNoTvHbFhGRuEbM.png\"\u003E\u003C/figure\u003E\u003Cp\u003EIf you’re using a device that you want to be able to access our server, you can repeat the installation as described above, adapting it to the specific device you’re using. Once you’ve successfully installed and logged in, the new device should appear on the dashboard.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e5013a9c750\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EHey folks, some time ago, I encountered an issue when trying to access my private server from a distant location. Many of us might immediately think of using ngrok, a popular service that connects our local server to the public by accessing the domain provided by ngrok (though it’s customizable, it’s not free). However, have you ever felt that there might be another more robust and suitable option for this need?\u003C/p\u003E\n\u003Cp\u003EOut of curiosity, I began looking for ngrok alternatives that might not be widely known. During this search, I discovered an enlightening solution named Tailscale.\u003C/p\u003E\n\u003Cp\u003EIn this article, I’d like to share the story of this fascinating discovery. Why Tailscale could be an impressive choice, how it operates so well, and of course, how this can be a revelation for digital buddies like you. So, are you ready to dive into this story with me? Let’s kick off our digital adventure this time, shall we?\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*cQBpxQwqa0cm9rHF.png\"\u003E\u003C/figure\u003E\u003Cp\u003ETailscale is a platform that allows you to easily create a virtual private network (VPN) based on WireGuard technology. Unlike traditional VPNs which require complex configurations, Tailscale offers a simpler approach, making it easier to set up and use.\u003C/p\u003E\n\u003Cp\u003ESome advantages of Tailscale include:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003ETailscale utilizes WireGuard, a modern VPN protocol designed for high performance and strong security.\u003C/li\u003E\n\u003Cli\u003EIt employs authentication based on Google, Microsoft, or other accounts for setup automation and access management.\u003C/li\u003E\n\u003Cli\u003EThere’s no need for a central server or complicated NAT traversal. Each node communicates directly with another node when possible.\u003C/li\u003E\n\u003Cli\u003EAll traffic is encrypted, and each node’s authentication is securely conducted.\u003C/li\u003E\n\u003Cli\u003EIt’s available for various operating systems including Linux, Windows, macOS, iOS, and Android.”\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003ETailscale Installation Steps (for Ubuntu server):\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EFirst, download Tailscale here: \u003Ca href=\"https://tailscale.com/download/\"\u003Ehttps://tailscale.com/download/\u003C/a\u003E\n\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*NvWnoSMP6ntwUQ2Z.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\n\u003Cli\u003EYou can install according to the device you are using.\u003C/li\u003E\n\u003Cli\u003EDon’t forget to log in / register for a Tailscale account.\u003C/li\u003E\n\u003Cli\u003EIt’s recommended to use SSO (Single Sign-On).\u003C/li\u003E\n\u003Cli\u003ERun Tailscale and then log in through the browser using the command below:\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cblockquote\u003Esudo tailscale up\u003C/blockquote\u003E\n\u003Cul\u003E\u003Cli\u003EIf ‘Success’ appears, then it has been successful. Check on the dashboard at \u003Ca href=\"https://login.tailscale.com/admin/machines\"\u003Ehttps://login.tailscale.com/admin/machines\u003C/a\u003E It should appear somewhat like this:\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*ztNoTvHbFhGRuEbM.png\"\u003E\u003C/figure\u003E\u003Cp\u003EIf you’re using a device that you want to be able to access our server, you can repeat the installation as described above, adapting it to the specific device you’re using. Once you’ve successfully installed and logged in, the new device should appear on the dashboard.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e5013a9c750\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "ubuntu",
          "wireguard",
          "tailscale",
          "private-vpn",
          "servers"
        ]
      },
      {
        "title": "Cara Akses Server Local Dari Public Dengan Private VPN (Indonesian version)",
        "pubDate": "2023-08-08 17:51:11",
        "link": "https://nasriadzlani.medium.com/cara-akses-server-local-dari-public-dengan-private-vpn-indonesian-version-99befb086336?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/99befb086336",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EHello ges, beberapa waktu lalu, saya menemui kendala saat ingin mengakses server pribadi saya dari lokasi yang jauh. Banyak di antara kita mungkin langsung terpikir untuk menggunakan ngrok, sebuah layanan populer yang menghubungkan server lokal kita ke public dengan mengakses domain yang disediain sama ngroknya, (walau bisa custom, tapi bayar hehe ). Namun, apakah kamu pernah merasa bahwa mungkin ada opsi lain yang lebih mantap dan sesuai dengan kebutuhan ini?\u003C/p\u003E\n\u003Cp\u003EKarena iseng, saya mulai mencari alternatif ngrok yang mungkin belum banyak diketahui. Dalam pencarian tersebut, saya menemukan sebuah pencerahan, bernama T\u003Cstrong\u003Eailscale\u003C/strong\u003E.\u003C/p\u003E\n\u003Cp\u003EDalam tulisan ini, saya ingin berbagi cerita tentang penemuan menarik ini. Mengapa tailscale bisa menjadi pilihan yang mengesankan, bagaimana ia bekerja dengan yahud, dan tentunya bagaimana hal ini dapat menjadi pencerahan bagi teman-teman digital seperti kamu. So, siap untuk menyelami kisah ini bersamaku? Mari kita mulai petualangan digital kita kali ini, anjay.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*eLihxRS13hFrCc1A.png\"\u003E\u003Cfigcaption\u003ETail Scale Logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003ETailscale\u003C/strong\u003E adalah sebuah platform yang memungkinkan Anda untuk membuat jaringan pribadi virtual (VPN) dengan mudah berdasarkan teknologi WireGuard. Tidak seperti VPN tradisional yang memerlukan konfigurasi yang kompleks, Tailscale menawarkan pendekatan yang lebih sederhana, sehingga membuatnya lebih mudah untuk di-setup dan digunakan.\u003C/p\u003E\n\u003Cp\u003EBeberapa keunggulan dari tailscale:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003ETailscale menggunakan WireGuard, protokol VPN modern yang dirancang untuk kinerja tinggi dan keamanan yang kuat\u003C/li\u003E\n\u003Cli\u003EMenggunakan autentikasi berbasis akun Google, Microsoft, atau lainnya untuk otomatisasi setup dan manajemen akses.\u003C/li\u003E\n\u003Cli\u003ETidak perlu server pusat atau NAT traversal yang rumit. Setiap node berkomunikasi langsung dengan node lain jika memungkinkan.\u003C/li\u003E\n\u003Cli\u003ESeluruh lalu lintas dienkripsi dan otentikasi setiap node dilakukan dengan aman.\u003C/li\u003E\n\u003Cli\u003ETersedia untuk berbagai sistem operasi termasuk Linux, Windows, macOS, iOS, dan Android.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EStep Instalasi tailscale (case ubuntu server):\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EDownload dulu tailscale disini : \u003Ca href=\"https://tailscale.com/download/\"\u003Ehttps://tailscale.com/download/\u003C/a\u003E\n\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rO09QwW7OfrQbmyh0CqFvg.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\n\u003Cli\u003EKamu bisa instal sesuai device yang kamu gunakan.\u003C/li\u003E\n\u003Cli\u003EJangan lupa login / register akun tailscale\u003C/li\u003E\n\u003Cli\u003EDisarankan pakai sso\u003C/li\u003E\n\u003Cli\u003Erunning tailscale lalu login lewat browser dengan command dibawah\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cblockquote\u003Esudo tailscale up\u003C/blockquote\u003E\n\u003Cul\u003E\n\u003Cli\u003Ekalau muncul Success. maka sudah berhasil, dan cek di dashboard \u003Ca href=\"https://login.tailscale.com/admin/machines\"\u003Ehttps://login.tailscale.com/admin/machines\u003C/a\u003E\n\u003C/li\u003E\n\u003Cli\u003Eharusnya muncul hampir seperti ini\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6bePqDQ27oqdipi_YwNDJA.png\"\u003E\u003C/figure\u003E\u003Cp\u003EKalau dari device yang ingin bisa mengakses server kita, bisa lakukan lagi instalasi seperti diatas, dengan disesuaikan dengan device yang mau mengkases, Jika suda berhasil instal dan login, harusnya device baru muncul di dashboard.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=99befb086336\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EHello ges, beberapa waktu lalu, saya menemui kendala saat ingin mengakses server pribadi saya dari lokasi yang jauh. Banyak di antara kita mungkin langsung terpikir untuk menggunakan ngrok, sebuah layanan populer yang menghubungkan server lokal kita ke public dengan mengakses domain yang disediain sama ngroknya, (walau bisa custom, tapi bayar hehe ). Namun, apakah kamu pernah merasa bahwa mungkin ada opsi lain yang lebih mantap dan sesuai dengan kebutuhan ini?\u003C/p\u003E\n\u003Cp\u003EKarena iseng, saya mulai mencari alternatif ngrok yang mungkin belum banyak diketahui. Dalam pencarian tersebut, saya menemukan sebuah pencerahan, bernama T\u003Cstrong\u003Eailscale\u003C/strong\u003E.\u003C/p\u003E\n\u003Cp\u003EDalam tulisan ini, saya ingin berbagi cerita tentang penemuan menarik ini. Mengapa tailscale bisa menjadi pilihan yang mengesankan, bagaimana ia bekerja dengan yahud, dan tentunya bagaimana hal ini dapat menjadi pencerahan bagi teman-teman digital seperti kamu. So, siap untuk menyelami kisah ini bersamaku? Mari kita mulai petualangan digital kita kali ini, anjay.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*eLihxRS13hFrCc1A.png\"\u003E\u003Cfigcaption\u003ETail Scale Logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003E\u003Cstrong\u003ETailscale\u003C/strong\u003E adalah sebuah platform yang memungkinkan Anda untuk membuat jaringan pribadi virtual (VPN) dengan mudah berdasarkan teknologi WireGuard. Tidak seperti VPN tradisional yang memerlukan konfigurasi yang kompleks, Tailscale menawarkan pendekatan yang lebih sederhana, sehingga membuatnya lebih mudah untuk di-setup dan digunakan.\u003C/p\u003E\n\u003Cp\u003EBeberapa keunggulan dari tailscale:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003ETailscale menggunakan WireGuard, protokol VPN modern yang dirancang untuk kinerja tinggi dan keamanan yang kuat\u003C/li\u003E\n\u003Cli\u003EMenggunakan autentikasi berbasis akun Google, Microsoft, atau lainnya untuk otomatisasi setup dan manajemen akses.\u003C/li\u003E\n\u003Cli\u003ETidak perlu server pusat atau NAT traversal yang rumit. Setiap node berkomunikasi langsung dengan node lain jika memungkinkan.\u003C/li\u003E\n\u003Cli\u003ESeluruh lalu lintas dienkripsi dan otentikasi setiap node dilakukan dengan aman.\u003C/li\u003E\n\u003Cli\u003ETersedia untuk berbagai sistem operasi termasuk Linux, Windows, macOS, iOS, dan Android.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EStep Instalasi tailscale (case ubuntu server):\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EDownload dulu tailscale disini : \u003Ca href=\"https://tailscale.com/download/\"\u003Ehttps://tailscale.com/download/\u003C/a\u003E\n\u003C/li\u003E\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rO09QwW7OfrQbmyh0CqFvg.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\n\u003Cli\u003EKamu bisa instal sesuai device yang kamu gunakan.\u003C/li\u003E\n\u003Cli\u003EJangan lupa login / register akun tailscale\u003C/li\u003E\n\u003Cli\u003EDisarankan pakai sso\u003C/li\u003E\n\u003Cli\u003Erunning tailscale lalu login lewat browser dengan command dibawah\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cblockquote\u003Esudo tailscale up\u003C/blockquote\u003E\n\u003Cul\u003E\n\u003Cli\u003Ekalau muncul Success. maka sudah berhasil, dan cek di dashboard \u003Ca href=\"https://login.tailscale.com/admin/machines\"\u003Ehttps://login.tailscale.com/admin/machines\u003C/a\u003E\n\u003C/li\u003E\n\u003Cli\u003Eharusnya muncul hampir seperti ini\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6bePqDQ27oqdipi_YwNDJA.png\"\u003E\u003C/figure\u003E\u003Cp\u003EKalau dari device yang ingin bisa mengakses server kita, bisa lakukan lagi instalasi seperti diatas, dengan disesuaikan dengan device yang mau mengkases, Jika suda berhasil instal dan login, harusnya device baru muncul di dashboard.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=99befb086336\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "servers",
          "private-vpn",
          "ubuntu",
          "vpn",
          "tailscale"
        ]
      },
      {
        "title": "Install Kafka Cluster (3 Node) In Docker Compose with UI (Ubuntu)",
        "pubDate": "2023-08-01 09:51:16",
        "link": "https://nasriadzlani.medium.com/install-kafka-cluster-3-node-in-docker-compose-with-ui-ubuntu-264af0566290?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/264af0566290",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*8PyBStycrazhmir_mm7fQw.jpeg\"\u003E\u003Cfigcaption\u003Ekafka logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch3\u003E\u003Cstrong\u003EIntroduce\u003C/strong\u003E\u003C/h3\u003E\n\u003Cp\u003EKafka is a powerful and widely adopted distributed streaming platform that has revolutionized the way organizations handle real-time data processing. Developed by LinkedIn and later open-sourced by the Apache Software Foundation, Kafka is designed to handle high-throughput, fault-tolerant, and scalable data streams. In this article, we will delve into the core concepts and architecture of Kafka, explore its key features, and understand how it has become a fundamental component of modern data-driven applications.\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003ECore Concepts:\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003ETopics: In Kafka, data is organized into topics, which represent individual streams of records. A topic can be considered as a category or feed where data producers publish messages, and data consumers subscribe to these topics to process the messages in real-time.\u003C/li\u003E\n\u003Cli\u003EPartitions: Each topic is divided into one or more partitions to achieve horizontal scalability. Partitions allow Kafka to distribute the data across multiple servers, enabling parallel processing and load balancing. Each partition is an ordered and immutable sequence of records.\u003C/li\u003E\n\u003Cli\u003EProducers: Producers are applications responsible for publishing data to Kafka topics. They write data records to specific partitions within topics based on the partitioning strategy defined.\u003C/li\u003E\n\u003Cli\u003EConsumers: Consumers are applications that subscribe to one or more Kafka topics and process the data published by producers. Kafka supports both traditional “pull-based” and newer “push-based” consumer models.\u003C/li\u003E\n\u003Cli\u003EBrokers: Kafka is implemented as a distributed system, and the individual nodes in the cluster are called brokers. Brokers are responsible for handling data replication, storage, and serving consumer requests.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E2. Architecture:\u003C/p\u003E\n\u003Cp\u003EKafka’s architecture is based on a distributed publish-subscribe model. Producers send data to Kafka brokers, and consumers read data from brokers. The key components in the Kafka architecture include:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EZooKeeper: Kafka relies on Apache ZooKeeper to manage the distributed coordination of brokers and maintain metadata about topics, partitions, and consumer groups.\u003C/li\u003E\n\u003Cli\u003EBrokers: Kafka brokers form the core of the system. They store and manage data across partitions, handle replication, and facilitate communication with producers and consumers.\u003C/li\u003E\n\u003Cli\u003EConsumer Groups: Consumers can be organized into consumer groups, where each group shares the load of processing data from a topic. This allows parallel processing and enables scaling of consumer applications.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E3. Key Features:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EScalability: Kafka’s distributed nature allows it to scale horizontally across multiple servers, accommodating increasing data volumes without sacrificing performance.\u003C/li\u003E\n\u003Cli\u003EFault Tolerance: By replicating data across multiple brokers, Kafka ensures fault tolerance. If a broker fails, another broker within the cluster can take over its responsibilities seamlessly.\u003C/li\u003E\n\u003Cli\u003EDurability: Kafka persists data for a configurable period, providing data durability and allowing consumers to replay historical data as needed.\u003C/li\u003E\n\u003Cli\u003ELow Latency: Kafka’s design prioritizes low-latency data streaming, making it ideal for real-time applications and event-driven architectures.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E\u003Cstrong\u003EHow To Install On Docker Compose\u003C/strong\u003E\u003C/h3\u003E\n\u003Cp\u003ERequirements\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EDocker + Docker Compose\u003C/li\u003E\n\u003Cli\u003ESpec Requirements can check this article \u003Ca href=\"https://docs.confluent.io/platform/current/installation/system-requirements.html\"\u003Ehttps://docs.confluent.io/platform/current/installation/system-requirements.html\u003C/a\u003E\n\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EStep\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EInstall Zookeeper and kafka nodes (3 node)\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003Eversion: '2.1'\u003Cbr\u003E\u003Cbr\u003Eservices:\u003Cbr\u003E  zoo1:\u003Cbr\u003E    image: confluentinc/cp-zookeeper:7.3.2\u003Cbr\u003E    hostname: zoo1\u003Cbr\u003E    container_name: zoo1\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"2181:2181\"\u003Cbr\u003E    environment:\u003Cbr\u003E      ZOOKEEPER_CLIENT_PORT: 2181\u003Cbr\u003E      ZOOKEEPER_SERVER_ID: 1\u003Cbr\u003E      ZOOKEEPER_SERVERS: zoo1:2888:3888\u003Cbr\u003E\u003Cbr\u003E\u003Cbr\u003E  kafka1:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka1\u003Cbr\u003E    container_name: kafka1\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9092:9092\"\u003Cbr\u003E      - \"29092:29092\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 1\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003Cbr\u003E\u003Cbr\u003E  kafka2:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka2\u003Cbr\u003E    container_name: kafka2\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9093:9093\"\u003Cbr\u003E      - \"29093:29093\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 2\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003Cbr\u003E\u003Cbr\u003E\u003Cbr\u003E  kafka3:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka3\u003Cbr\u003E    container_name: kafka3\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9094:9094\"\u003Cbr\u003E      - \"29094:29094\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,DOCKER://host.docker.internal:29094\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 3\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003C/pre\u003E\n\u003Cul\u003E\n\u003Cli\u003Esave this file to docker-compose.yml on your server directory\u003C/li\u003E\n\u003Cli\u003Ecd into your directory\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cpre\u003Edocker compose up -d\u003C/pre\u003E\n\u003Cul\u003E\u003Cli\u003Ewaiting to show like\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003E ✔ Network kafka_default  Created                                                                                                                                                                                                        0.3s \u003Cbr\u003E ✔ Container zoo1         Started                                                                                                                                                                                                       12.0s \u003Cbr\u003E ✔ Container kafka1       Started                                                                                                                                                                                                        2.9s \u003Cbr\u003E ✔ Container kafka3       Started                                                                                                                                                                                                        3.4s \u003Cbr\u003E ✔ Container kafka2       Started   \u003C/pre\u003E\n\u003Cp\u003EInstalling UI for manage your kafka\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003Erun this command\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003Edocker run -it -d -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true provectuslabs/kafka-ui\u003C/pre\u003E\n\u003Cul\u003E\n\u003Cli\u003Eopen on your browser \u003Ca href=\"http://10.100.1.115:8080/\"\u003Ehttp://&lt;your-ip&gt;:8080/\u003C/a\u003E\n\u003C/li\u003E\n\u003Cli\u003EClick configure new cluster\u003C/li\u003E\n\u003Cli\u003Eadd your cluster like this\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1D0RqDWznsaDRwLuFF4rnA.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\u003Cli\u003Eclick Submit\u003C/li\u003E\u003C/ul\u003E\n\u003Cp\u003EDone\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=264af0566290\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*8PyBStycrazhmir_mm7fQw.jpeg\"\u003E\u003Cfigcaption\u003Ekafka logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Ch3\u003E\u003Cstrong\u003EIntroduce\u003C/strong\u003E\u003C/h3\u003E\n\u003Cp\u003EKafka is a powerful and widely adopted distributed streaming platform that has revolutionized the way organizations handle real-time data processing. Developed by LinkedIn and later open-sourced by the Apache Software Foundation, Kafka is designed to handle high-throughput, fault-tolerant, and scalable data streams. In this article, we will delve into the core concepts and architecture of Kafka, explore its key features, and understand how it has become a fundamental component of modern data-driven applications.\u003C/p\u003E\n\u003Col\u003E\u003Cli\u003ECore Concepts:\u003C/li\u003E\u003C/ol\u003E\n\u003Cul\u003E\n\u003Cli\u003ETopics: In Kafka, data is organized into topics, which represent individual streams of records. A topic can be considered as a category or feed where data producers publish messages, and data consumers subscribe to these topics to process the messages in real-time.\u003C/li\u003E\n\u003Cli\u003EPartitions: Each topic is divided into one or more partitions to achieve horizontal scalability. Partitions allow Kafka to distribute the data across multiple servers, enabling parallel processing and load balancing. Each partition is an ordered and immutable sequence of records.\u003C/li\u003E\n\u003Cli\u003EProducers: Producers are applications responsible for publishing data to Kafka topics. They write data records to specific partitions within topics based on the partitioning strategy defined.\u003C/li\u003E\n\u003Cli\u003EConsumers: Consumers are applications that subscribe to one or more Kafka topics and process the data published by producers. Kafka supports both traditional “pull-based” and newer “push-based” consumer models.\u003C/li\u003E\n\u003Cli\u003EBrokers: Kafka is implemented as a distributed system, and the individual nodes in the cluster are called brokers. Brokers are responsible for handling data replication, storage, and serving consumer requests.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E2. Architecture:\u003C/p\u003E\n\u003Cp\u003EKafka’s architecture is based on a distributed publish-subscribe model. Producers send data to Kafka brokers, and consumers read data from brokers. The key components in the Kafka architecture include:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EZooKeeper: Kafka relies on Apache ZooKeeper to manage the distributed coordination of brokers and maintain metadata about topics, partitions, and consumer groups.\u003C/li\u003E\n\u003Cli\u003EBrokers: Kafka brokers form the core of the system. They store and manage data across partitions, handle replication, and facilitate communication with producers and consumers.\u003C/li\u003E\n\u003Cli\u003EConsumer Groups: Consumers can be organized into consumer groups, where each group shares the load of processing data from a topic. This allows parallel processing and enables scaling of consumer applications.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E3. Key Features:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EScalability: Kafka’s distributed nature allows it to scale horizontally across multiple servers, accommodating increasing data volumes without sacrificing performance.\u003C/li\u003E\n\u003Cli\u003EFault Tolerance: By replicating data across multiple brokers, Kafka ensures fault tolerance. If a broker fails, another broker within the cluster can take over its responsibilities seamlessly.\u003C/li\u003E\n\u003Cli\u003EDurability: Kafka persists data for a configurable period, providing data durability and allowing consumers to replay historical data as needed.\u003C/li\u003E\n\u003Cli\u003ELow Latency: Kafka’s design prioritizes low-latency data streaming, making it ideal for real-time applications and event-driven architectures.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3\u003E\u003Cstrong\u003EHow To Install On Docker Compose\u003C/strong\u003E\u003C/h3\u003E\n\u003Cp\u003ERequirements\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EDocker + Docker Compose\u003C/li\u003E\n\u003Cli\u003ESpec Requirements can check this article \u003Ca href=\"https://docs.confluent.io/platform/current/installation/system-requirements.html\"\u003Ehttps://docs.confluent.io/platform/current/installation/system-requirements.html\u003C/a\u003E\n\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EStep\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003EInstall Zookeeper and kafka nodes (3 node)\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003Eversion: '2.1'\u003Cbr\u003E\u003Cbr\u003Eservices:\u003Cbr\u003E  zoo1:\u003Cbr\u003E    image: confluentinc/cp-zookeeper:7.3.2\u003Cbr\u003E    hostname: zoo1\u003Cbr\u003E    container_name: zoo1\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"2181:2181\"\u003Cbr\u003E    environment:\u003Cbr\u003E      ZOOKEEPER_CLIENT_PORT: 2181\u003Cbr\u003E      ZOOKEEPER_SERVER_ID: 1\u003Cbr\u003E      ZOOKEEPER_SERVERS: zoo1:2888:3888\u003Cbr\u003E\u003Cbr\u003E\u003Cbr\u003E  kafka1:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka1\u003Cbr\u003E    container_name: kafka1\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9092:9092\"\u003Cbr\u003E      - \"29092:29092\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 1\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003Cbr\u003E\u003Cbr\u003E  kafka2:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka2\u003Cbr\u003E    container_name: kafka2\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9093:9093\"\u003Cbr\u003E      - \"29093:29093\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 2\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003Cbr\u003E\u003Cbr\u003E\u003Cbr\u003E  kafka3:\u003Cbr\u003E    image: confluentinc/cp-kafka:7.3.2\u003Cbr\u003E    hostname: kafka3\u003Cbr\u003E    container_name: kafka3\u003Cbr\u003E    ports:\u003Cbr\u003E      - \"9094:9094\"\u003Cbr\u003E      - \"29094:29094\"\u003Cbr\u003E    environment:\u003Cbr\u003E      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,DOCKER://host.docker.internal:29094\u003Cbr\u003E      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT\u003Cbr\u003E      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\u003Cbr\u003E      KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\"\u003Cbr\u003E      KAFKA_BROKER_ID: 3\u003Cbr\u003E      KAFKA_LOG4J_LOGGERS: \"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\"\u003Cbr\u003E      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\u003Cbr\u003E      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\u003Cbr\u003E    depends_on:\u003Cbr\u003E      - zoo1\u003C/pre\u003E\n\u003Cul\u003E\n\u003Cli\u003Esave this file to docker-compose.yml on your server directory\u003C/li\u003E\n\u003Cli\u003Ecd into your directory\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cpre\u003Edocker compose up -d\u003C/pre\u003E\n\u003Cul\u003E\u003Cli\u003Ewaiting to show like\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003E ✔ Network kafka_default  Created                                                                                                                                                                                                        0.3s \u003Cbr\u003E ✔ Container zoo1         Started                                                                                                                                                                                                       12.0s \u003Cbr\u003E ✔ Container kafka1       Started                                                                                                                                                                                                        2.9s \u003Cbr\u003E ✔ Container kafka3       Started                                                                                                                                                                                                        3.4s \u003Cbr\u003E ✔ Container kafka2       Started   \u003C/pre\u003E\n\u003Cp\u003EInstalling UI for manage your kafka\u003C/p\u003E\n\u003Cul\u003E\u003Cli\u003Erun this command\u003C/li\u003E\u003C/ul\u003E\n\u003Cpre\u003Edocker run -it -d -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true provectuslabs/kafka-ui\u003C/pre\u003E\n\u003Cul\u003E\n\u003Cli\u003Eopen on your browser \u003Ca href=\"http://10.100.1.115:8080/\"\u003Ehttp://&lt;your-ip&gt;:8080/\u003C/a\u003E\n\u003C/li\u003E\n\u003Cli\u003EClick configure new cluster\u003C/li\u003E\n\u003Cli\u003Eadd your cluster like this\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1D0RqDWznsaDRwLuFF4rnA.png\"\u003E\u003C/figure\u003E\u003Cul\u003E\u003Cli\u003Eclick Submit\u003C/li\u003E\u003C/ul\u003E\n\u003Cp\u003EDone\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=264af0566290\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "kafka",
          "apache-kafka",
          "data-engineer",
          "confluent-kafka",
          "queue"
        ]
      },
      {
        "title": "Berkenalan Dengan Solana dan Proof Of History",
        "pubDate": "2022-10-17 06:06:21",
        "link": "https://nasriadzlani.medium.com/berkenalan-dengan-solana-dan-proof-of-history-d2bbfd473fbd?source=rss-6433ae88e0c6------2",
        "guid": "https://medium.com/p/d2bbfd473fbd",
        "author": "Nasri Adzlani",
        "thumbnail": "",
        "description": "\n\u003Cp\u003EHalo guys, salam kenal semuanya, perkenalkan namaku nasri, dan sekarang sedang mempelajari sedikit tentang blockchain, salah satunya yaitu solana, dan kita akan bahas disini.\u003C/p\u003E\n\u003Cp\u003EKita mulai dengan sedikit pengenalan tentang solana, solana merupakan project opensource yang menggunakan teknologi blockchain mereka sendiri yang banyak digunakan untuk prject Decentralize Finance (DeFi) maupun NFT, walau banyak juga project yang menggunakan solana sesuai case mereka.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*rYoEr311GA9JEb67.png\"\u003E\u003Cfigcaption\u003Esolana logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003ESolana menggunakan konsensus proof of stake, kalau mau lebih detail tentang Proof of Stake (PoS) bisa baca \u003Ca href=\"https://nasriadzlani.medium.com/berkenalan-dengan-proof-of-stake-7030792103b8\"\u003E\u003Cstrong\u003Edisini\u003C/strong\u003E\u003C/a\u003E\u003Cstrong\u003E.\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EWalau solana menggunakan PoS sebagai konsensus, tapi kecepatan transaksi di solana bisa mencapai 50.000+ transaksi per detik. yang sangat berbeda jauh dengan kecepatan validasi transaksi di konsensus PoS lainnya, semua itu bisa dijawab dengan\u003Cstrong\u003E Proof of History (PoH).\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EProof of History ternyata\u003Cstrong\u003E bukan termasuk konsensus,\u003C/strong\u003E walau agak membingungkan karena menggunakan “Proof of” yang mirip dengan nama konsensus lainnya, dan PoH ini hanya digunakan sebagai pendukung Proof of stake yang solana pakai, untuk beberapa permasalahan yang dialami konsensus lainnya,\u003C/p\u003E\n\u003Cp\u003Ekita ambil analogi untuk lebih memahaminya, “Kalian tau talky walky? yang kalau mau bicara harus menekan tombol, dan target hanya bisa mendengar agar tidak tabrakan suaranya kalau ngomong secara bareng”, hal ini hampir sama dengan yang yang dialami oleh konsensus lain. dian di blockchain kalau ada yang menyelesaikan transaksi di waktu yang sama maka akan membuat lebih dari 1 block baru, tapi endingnya hanya satu block yang dipakai, karena itu terjadi maka akan ada resource yang terbuang, dan memperlama proses transaksi.\u003C/p\u003E\n\u003Cp\u003Euntuk mengoptimalisasi makanya diciptakan PoH ini yang digunakan dengan urutan waktu, makanya disebut “\u003Cstrong\u003EHistory\u003C/strong\u003E” jadi waktu transaksi tidak ada yang tabrakan, dan proses validasi sesuai dengan urutan.\u003C/p\u003E\n\u003Cp\u003Eselain PoH, solana juga didukung oleh sebuah teknologi bernama “\u003Cstrong\u003EPIPELINE\u003C/strong\u003E” yang dipakai untuk optimasi pemrosesan data yang dikirim ke beberapa hardware berbeda yang dilakukan secara cepat kesemua node didalam jaringan, dan juga solana menggunakan “L\u003Cstrong\u003Eower PBP\u003C/strong\u003E” yang digunakan sebagai jam kriptografi untuk pencatatan waktu dari PoH yang tanpa fee, dan ada juga “\u003Cstrong\u003EGULFSTREAM\u003C/strong\u003E” yang sebagai penyimpanan cache history yang disimpan ke RAM, dan diteruskan ke jaringan solana agar prosesnya sangat cepat, karena ini juga jumlah transaksi solana bisa paling banyak daripada blockchain lainnya.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d2bbfd473fbd\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "content": "\n\u003Cp\u003EHalo guys, salam kenal semuanya, perkenalkan namaku nasri, dan sekarang sedang mempelajari sedikit tentang blockchain, salah satunya yaitu solana, dan kita akan bahas disini.\u003C/p\u003E\n\u003Cp\u003EKita mulai dengan sedikit pengenalan tentang solana, solana merupakan project opensource yang menggunakan teknologi blockchain mereka sendiri yang banyak digunakan untuk prject Decentralize Finance (DeFi) maupun NFT, walau banyak juga project yang menggunakan solana sesuai case mereka.\u003C/p\u003E\n\u003Cfigure\u003E\u003Cimg alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*rYoEr311GA9JEb67.png\"\u003E\u003Cfigcaption\u003Esolana logo\u003C/figcaption\u003E\u003C/figure\u003E\u003Cp\u003ESolana menggunakan konsensus proof of stake, kalau mau lebih detail tentang Proof of Stake (PoS) bisa baca \u003Ca href=\"https://nasriadzlani.medium.com/berkenalan-dengan-proof-of-stake-7030792103b8\"\u003E\u003Cstrong\u003Edisini\u003C/strong\u003E\u003C/a\u003E\u003Cstrong\u003E.\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EWalau solana menggunakan PoS sebagai konsensus, tapi kecepatan transaksi di solana bisa mencapai 50.000+ transaksi per detik. yang sangat berbeda jauh dengan kecepatan validasi transaksi di konsensus PoS lainnya, semua itu bisa dijawab dengan\u003Cstrong\u003E Proof of History (PoH).\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EProof of History ternyata\u003Cstrong\u003E bukan termasuk konsensus,\u003C/strong\u003E walau agak membingungkan karena menggunakan “Proof of” yang mirip dengan nama konsensus lainnya, dan PoH ini hanya digunakan sebagai pendukung Proof of stake yang solana pakai, untuk beberapa permasalahan yang dialami konsensus lainnya,\u003C/p\u003E\n\u003Cp\u003Ekita ambil analogi untuk lebih memahaminya, “Kalian tau talky walky? yang kalau mau bicara harus menekan tombol, dan target hanya bisa mendengar agar tidak tabrakan suaranya kalau ngomong secara bareng”, hal ini hampir sama dengan yang yang dialami oleh konsensus lain. dian di blockchain kalau ada yang menyelesaikan transaksi di waktu yang sama maka akan membuat lebih dari 1 block baru, tapi endingnya hanya satu block yang dipakai, karena itu terjadi maka akan ada resource yang terbuang, dan memperlama proses transaksi.\u003C/p\u003E\n\u003Cp\u003Euntuk mengoptimalisasi makanya diciptakan PoH ini yang digunakan dengan urutan waktu, makanya disebut “\u003Cstrong\u003EHistory\u003C/strong\u003E” jadi waktu transaksi tidak ada yang tabrakan, dan proses validasi sesuai dengan urutan.\u003C/p\u003E\n\u003Cp\u003Eselain PoH, solana juga didukung oleh sebuah teknologi bernama “\u003Cstrong\u003EPIPELINE\u003C/strong\u003E” yang dipakai untuk optimasi pemrosesan data yang dikirim ke beberapa hardware berbeda yang dilakukan secara cepat kesemua node didalam jaringan, dan juga solana menggunakan “L\u003Cstrong\u003Eower PBP\u003C/strong\u003E” yang digunakan sebagai jam kriptografi untuk pencatatan waktu dari PoH yang tanpa fee, dan ada juga “\u003Cstrong\u003EGULFSTREAM\u003C/strong\u003E” yang sebagai penyimpanan cache history yang disimpan ke RAM, dan diteruskan ke jaringan solana agar prosesnya sangat cepat, karena ini juga jumlah transaksi solana bisa paling banyak daripada blockchain lainnya.\u003C/p\u003E\n\u003Cimg src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d2bbfd473fbd\" width=\"1\" height=\"1\" alt=\"\"\u003E\n",
        "enclosure": {
  
        },
        "categories": [
          "solanas",
          "pohs",
          "proof-of-history",
          "proof-of-stake",
          "blockchain"
        ]
      }
    ]
  }